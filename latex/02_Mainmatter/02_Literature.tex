\chapter{Literature Review}
\label{chap:lit} 
		 	
 	The literature review proceeds as follows. TODO update. First, I review the theory behind stock returns to explain the economic motivation of predictors used in this thesis. Second, I motivate the use of ML in stock returns prediction and review how ML interpretability has advanced in this field so far. Finally, I review ML interpretability literature with the aim of choosing a suitable feature importance measure.
 	
 	\section {Approaches to Modeling Stock Returns}
 	
 	There are two major approaches in literature to modeling stock returns: factor models and characteristics models. Factor models, exemplified by \cite{fama1996multifactor, fama2015five}, model the returns as  
 	
 	\begin{equation}
 		E_t(R^i_{t+1}) = R^f_{t+1} + \beta_{1,i,t} \lambda_{1,t} + \beta_{2,i,t} \lambda_{2,t} + \ldots +  \beta_{K,i,t} \lambda_{K,t} 
 	\end{equation}
 	
 	The \textit{factor loadings} $\vec{\beta}'_{i,t}$ of size $1\times K$ are exposures to $K$ risk factors.  They represent the amount of risk inherent in asset $i$ due to its correlation to the corresponding risk factor. The $\vec{\lambda}_{t}$ of size $K \times 1$ are interpretable as the risk prices at time $t$ \citep{kelly2019characteristics}. (Note that the CAPM and consumption-based models are special cases of the multiple-factor models, where $K$, the number of factors, is 1.) 
 	
 	
 	
 	\section{Modeling Stock Returns With Characteristics}
	 	
	 	\setlength{\epigraphwidth}{0.8\textwidth}
	 	\epigraph{We have a lot of questions to answer: First, which characteristics really provide	independent information about average returns?}{\cite{cochrane2011presidential}}
			
		One of the common approaches to model stock returns is to use firm-specific variables, called characteristics, as predictors of the return. These characteristics are time-variant, for example, if net income is used as a characteristic, its values for a given company change as often as the firm in question issues its financial reports. To produce a prediction of a stock's return for the next month, the currently available characteristics for that firm enter the prediction function $f$, which outputs the predicted return. In equation, this modeling approach can be summarized as 
		
		\begin{equation}
			E_t(r^i_{t+1}) = f(\vec{x}'_{i,t})
			\label{eq:expected_return}
		\end{equation}
	
		where the $E_t(r^i_{t+1})$ is the expected return of stock $i$ in the next period ($t+1$) as a function $f$ of the characteristics of stock $i$ at time $t$ ($x_{i,t}$). Empirical examples include e.g.,  \cite{gu2020empirical, tobek2020does, bryzgalova2019forest}. This is also the approach taken by this thesis. Equation \ref{eq:expected_return} shows that the task of the researcher is to determine which characteristics ($\vec{x}$) to use, and the general functional form $f$ in which they should interact to produce the prediction. The parameters which pin down the particular manifestation of the functional form are then estimated empirically using data, which completes the model. The choice of the characteristics and of the functional form is now discussed in turn using existing literature. 
		
		\subsection{Choosing the Characteristics}	
			
			Literature has accumulated hundreds of variables that purportedly predict stock returns. \cite{harvey2016and} count 313 variables, considering the top journals only and calling the count "surely too low". Indeed, \cite{cochrane2011presidential} refers to the state of asset pricing as a "zoo". Actually, many of these published predictors are spurious. As \cite[p.~5]{harvey2016and} put it rather famously: "most claimed research findings in financial economics are likely false". Main reasons for this include publication bias and data-snooping bias (multiple-hypothesis testing bias, in-sample overfitting) and lack of replication studies in finance \citep{harvey2016and, mclean2016does}. 12\% of variables cannot be replicated even in-sample, the rest is biased by about 10\% \citep{mclean2016does}. \cite{harvey2016and} show that out of 296 published significant factors 158, 142, 132 and 80 are false discoveries, the precise number depending on statistical framework used.
			
			\citeauthor{cochrane2011presidential} formulates the problem of separating the wheat from the chaff in stock returns prediction in his "multidimensional challenge" \citep[p.~1060]{cochrane2011presidential}: "We have a lot of questions to answer: First, which characteristics really provide independent information about average returns?" In response to this challenge, studies emerged that consider all the published characteristics in a single model, which allows to their effects to crowd each other out in a 'survival of the fittest', such as \cite{gu2020empirical} and \cite{tobek2020does}. 
			
			\cite{gu2020empirical} and \cite{tobek2020does} study 94 (153) characteristics proposed by prior literature and use them in various return-prediction models. Their results suggest that some categories of characteristics are consistently more important than others across time and model specifications. This finding presents a crucial stepping stone for this thesis, allowing it to select 30 most important variables from \cite{tobek2020does} as stock return predictors, which can be considered a distillation of the current literature's answer to the Cochrane's multidimensional challenge.\footnote{There are three advantages of \cite{tobek2020does} over \cite{gu2020empirical} for this purpose: First, like this thesis,  \cite{tobek2020does} use a liquid universe of stocks, which means that their results are less likely to be driven by transactions costs. Second, their data are global, rather than US only, which allows to uncover more generally applicable findings. Finally, they use larger set of characteristics (153 rather than 93), which makes it less likely that important information is omitted.} 
			
		\subsection{Economic Motivation of the Characteristics Used in This Thesis}
			
			All characteristics have economic motivation that suggests a particular mechanism of why they should predict stock returns. When we inspect this economic rationale, several broader categories emerge consistently. The remainder of this subsection reviews these economic mechanisms and at the same time presents the 30 variables used in this thesis as members of these broader categories. This is summarized in Table \ref{tab:characteristics_motivation}, which shows all characteristics used in this thesis (column Features), together with their authors and publishing journal, categorized by their economic motivation (column Category). Each category is discussed in detail its own paragraph below. 
			
			\begin{table}
				\resizebox{\textwidth}{!}{\input{Tables/characteristics_motivation.tex}}
				\caption{Economic Motivation of Predictors Used in This Thesis}
				\label{tab:characteristics_motivation}
				\medskip
				\small 
				The table shows all the characteristics (features) used in this thesis grouped by their underlying economic motivation as predictors of stock returns. Column Category gives the main economic motivation of the variable and is discussed in detail in the text of this subsection.
			\end{table}
			
			A first group of characteristics relates to how investors approach volatility. Traditional asset pricing theory assumes investor's approach to risk is symmetrical in gains and losses \citep{cochrane2009asset}. However, psychology suggests that people are at the same time risk-averse (case of insurance) and risk-loving (case of lottery) and have asymmetrical utility in gains and in losses \citep{kahneman2013prospect}. This is in contrast to the standard utility maximization assumption of the Consumption-Based model and Capital Asset Pricing Model discussed above. This has direct implications for stock returns: it seems that lottery-like stocks, stocks with right skew and stocks with low correlation with market volatility can afford to pay lower average returns in exchange for their appealing risk profile (predictors \textit{Maximum Return} \citep{bali2011maxing}, \textit{Coskewness} \citep{harvey2000conditional} and \textit{Idiosyncratic Risk} \citep{ang2006cross}, respectively).
			
			A second group of characteristics also relates to the behavioral science. It seems that investors are slow to revise their existing beliefs upon arrival of new information, which shows in stock returns as momentum (predictors \textit{52-Week High} \citep{george200452} and \textit{Lagged Momentum} \citep{novy2012momentum}), and when they \textit{do} revise their beliefs, they over-react, which shows as reversal -- periods of high returns followed by periods of low returns (predictors \textit{Short-Term Reversal} \citep{jegadeesh1990evidence} and \textit{Momentum-Reversal} \citep{jegadeesh1993returns}). 
			
			A third and fourth group of characteristics relate to liquidity. It appears that illiquid stocks (typically small stocks with low trading volume and large bid-ask spread) must offer a higher return to compensate for their higher trading costs (predictors \textit{Amihud's Measure} \citep{amihud2002illiquidity}, \textit{Liquidity Shocks} \citep{bali2013liquidity} and \textit{Volume over Market Value of Equity}  \citep{haugen1996commonality}). It seems that investors avoid illiquidity to the degree a that mere higher chance of a stock becoming illiquid commands a higher return (predictors \textit{Coefficient of Variation of Share Turnover} \citep{chordia2001trading} and \textit{Liquidity Beta 3} and \textit{Liquidity Beta 5}  \citep{acharya2005asset}).  
			
			A fifth group of characteristics focuses is due to limited attention of investors, which can result in mis-pricing. First, it seems that investors tend to overlook artificially bloated (manipulated or "dressed") balance sheets, and tend to over-price firms with high accruals (predictors \textit{Accruals} \citep{sloan1996create}, \textit{Net Operating Assets} \citep{hirshleifer2004investors}, \textit{Operating Profits to Assets} \citep{ball2016accruals}, and \textit{Change in Common Equity} \cite{richardson2006implications}). Similarly, they tend to under-value firms with high research and development costs (which are not present on the balance sheets) (predictor \textit{ RD Over Market Equity} \cite{chan2001stock}). Finally, limited attention of investors manifests in overlooking important decompositions of profit margin (predictor \textit{Profit Margin} \cite{soliman2008use}).
			
			A sixth group of characteristics reflects the empirically observed seasonal patterns in stock returns. While the theoretical underpinning of these characteristics is still a matter of academic debate, the phenomenon is very strong and robust empirically (\textit{Seasonality} predictors from \cite{heston2008seasonality}). 
			
			A seventh group of characteristics are proxies for the well-known value effect \cite{fama1993common}, which shows that empirically, firms with high Book to Market ratio tend to have higher returns. The predictors used here are \textit{ Duration of Equity }\citep{dechow2004implied}) and \textit{Leverage Component of Book to Price} \citep{penman2007book}, and their original papers discuss also the theoretical underpinnings of the phenomenon.
			
			Finally, financially constrained firms tend to co-move, and since financial constraints are typically market-wide during economic crises, this risk is not diversified, and therefore priced. The predictor \textit{Whited-Wu Index} \citep{whited2006financial} measures the extent to which a stock is impacted by financial constraints. 
		
		
		\subsection{Choosing the Functional Form}
		
			A second modeling decision to be made is choosing the general functional form $f$ in which the characteristics should interact in equation \ref{eq:expected_return}. The classical approach is to assume a linear function: 
			
			TODO describe 
			
			Second, the different models (gradient-boosting regression trees, random forests, and neural networks) agree on important variables, which allows this thesis to focus on neural networks (and its comparison to linear regression) without loss of generality.
	
 		
 		\section{Alternative Approaches}
 		
 			TODO rewrite
 			
	 		It is difficult to empirically estimate a multifactor model. The main challenge is that both $\vec{f}_{t+1}$ and $\vec{\beta}'_{i,t}$ are unobserved, so we have no directly observed variables at the right hand side of the estimated equation. Some researchers tackle the issue by using prior empirical knowledge to pre-specify the factors  \citep{fama1993common, fama2015five}, others use instrumental variable approach \citep{kelly2019characteristics} to uncover the unobserved factor loadings using firm-level variables, commonly called \textit{characteristics}, which serve as proxies for exposure to systematic risks \cite{kelly2019characteristics}:  
	 		
	 		\begin{align}
	 			E_t(R^i_{t+1}) = R^f_{t+1} + \vec{\beta}'_{i,t} \vec{\lambda}_{t}  \label{eq:ER_betalambda} \\
	 			R^i_{t+1} = \alpha_{i,t} + \vec{\beta}'_{i,t} \vec{f}_{t+1} + \epsilon_{i,t+1} \\
	 			\vec{\beta}'_{i,t} = g(\vec{z}'_{i,t}) \label{eq:instrumented_betas}
	 		\end{align}
	 		
	 		
	 		An important distinction between characteristics and factors is that characteristics are firm-specific variables, such as firm size, returns momentum or book-to-market ratio, while factors are common to all firms, such as the market return. Additionally, while characteristics proxy a firm's \textit{exposure} to risk (equation \ref{eq:instrumented_betas}), factors are the risk sources themselves. 
	 		
		 		  Consider the following basic asset-pricing equation \citep{cochrane2009asset}:
	 		
	 		\begin{equation}
	 			1 = E_t(m_{t+1} R_{t+1}^i ) \label{eq:1EmR}
	 		\end{equation} 
	 		
	 		In words, an investor pays 1 dollar now to collect $R_{t+1}^i$ dollars in the future. The payoff is called gross return and is the sum of future price and dividend $R_{t+1}^i = p_{t+1}^i+ d_{t+1}^i$.  Since the payoff does not come now ($t$), but in future ($t+1$), we discount it to present by the factor $m_{t+1}$, called the \textit{stochastic discount factor}. The term stochastic stands to express the idea that $m_{t+1}$ is not known with certainty at time $t$ \citep{cochrane2009asset}.
	 		Using the definition of covariance, we can rearrange \ref{eq:1EmR} \citep{cochrane2009asset}:
	 		
	 		\begin{equation}
	 			1 = E_t(m_{t+1}) E_t(R_{t+1}^i) + Cov_t(m_{t+1},R_{t+1}^i) 
	 		\end{equation} 
	 		
	 		Defining the \textit{risk-free gross return} as $R_{t+1}^f=\frac{1}{E_t(m_{t+1})}$ and using it to further rearrange \citep{cochrane2009asset}:
	 		
	 		\begin{equation}
	 			E_t(R_{t+1}^i) = \underbrace{R_{t+1}^f}_\text{risk-free return}  -  \underbrace{R_{t+1}^f Cov_t(m_{t+1},R_{t+1}^i)}_\text{risk adjustment} \label{eq:risk_adjustment}
	 		\end{equation} 
	 		
	 		This important result shows that expected return if stock $i$ can be decomposed into risk-free return and risk adjustment. Note that the risk adjustment occurs if and only if returns are correlated to the discount factor, so \textit{idiosyncratic} risk, that is, uncorrelated with the discount factor, is uncompensated. Returns positively correlated to the discount factor should be low, and vice versa. So to explain average returns, we "only" need to explain the returns' correlation to the discount factor $m_{t+1}$. 
	 		
	 		Further rearranging \ref{eq:risk_adjustment} provides one more insight \citep{cochrane2009asset}. Multiply both sides by $Var_t(m_{t+1})/Var_t(m_{t+1})$ to obtain:
	 		
	 		\begin{align}
	 			E_t(R_{t+1}^i) 
	 			& = {R_{t+1}^f} + 
	 			\underbrace{
	 				\frac{Cov_t(m_{t+1},R_{t+1}^i)}{Var_t(m_{t+1})}
	 			}_\text{$\beta_{i,t}$} 
	 			\cdot 
	 			\underbrace{
	 				\left(-\frac{Var_t(m_{t+1})}{E_t(m_{t+1})}\right)
	 			}_\text{$\lambda_{t}$} \label{eq:factor_model} \\
	 			& = \underbrace{{R_{t+1}^f}
	 			}_\text{risk-free return} 
	 			+ 
	 			\underbrace{\beta_{i,t} \cdot \lambda_{t}
	 			}_\text{risk adjustment} \label{eq:beta_reprezentation}
	 		\end{align} 
	 		
	 		This equation is so-called \textit{beta-representation} of \ref{eq:1EmR}. It shows that the risk-adjustment, the premium a stock pays for being correlated with the discount factor, can be decomposed into $\beta_{i,t}$ and $\lambda_t$. $\lambda_t$ is the volatility of the discount factor and it is unrelated to properties of asset $i$. It can be interpreted as the price of risk. $\beta_{i,t}$ can be interpreted as the amount of the risk inherent in asset $i$. 
	 		
	 		
	 				
	 		
	 		First multi-factor model is \cite{fama1996multifactor}:
	 		
	 		\begin{align}
	 			& E(R^i) = R^f + \vec{\beta}'_{i} \vec{\lambda} \\
	 			& R^i_{t+1} = \alpha_{i} + \vec{\beta}'_{i} \vec{f}_{t+1} + \epsilon_{i,t+1}
	 		\end{align}
	 		
	 		with factors  
	 		
	 		\begin{equation}
	 			\vec{f}_{t+1} = 
	 			\begin{pmatrix}
	 				R^M_{t+1}-R^f_{t+1} \\
	 				SMB_{t+1} \\
	 				HML_{t+1}				
	 			\end{pmatrix}
	 		\end{equation}
	 		
	 		
	 		The $R^M_{t+1}-R^f_{t+1}$ is the return on market portfolio above risk-free rate (as in CAPM), $SMB_{t+1}$ is the return on small-cap portfolio minus the return on big-cap portfolio, and $HML_{t+1}$ is the return on portfolio of stocks with small market values relative to book value, minus the return on portfolio of stocks with high market values relative to book value.
	 		
			
		\section{Machine Learning and Stock Returns}		
		
		\subsection{Why use Neural Networks for Stock Returns Prediction?}
			
			One of the reasons behind popularity of neural networks in stock returns prediction is that they offer superior performance. \cite{gu2020empirical} make a horse race of several ML and traditional methods. They show that neural networks predict stock returns better than any other model, both in explained variance and in profitability. They are far superior to linear models (linear regression with and without regularization, principal components regression, elastic net), and also beat other nonlinear models (generalized linear models, random forest, and gradient-boosted regression trees). \cite{tobek2020does} confirm these findings on international (as opposed to U.S.) data and liquid universe of stocks. \cite{gu2020empirical} statistically reject linear models in favor of the non-linear ones as those with better performance.
			
			Second, there is strong theoretical appeal to using neural networks as well. Prior research shows that linear models miss important information about stock returns by ignoring the interactions of the predictors. If a variable has a non-linear relationship with returns, the linear models can conclude there is no association, while in reality there is an important non-linear relationship. For example, in \cite{gu2020empirical}, linear models deem size and volatility unimportant predictors, while non-linear models find the contrary. A similar problem may arise if predictors are have zero association with return themselves, but not in interaction with other predictors. The nonlinearity seems to be a curcial aspect of stock return prediction: \cite{bryzgalova2019forest} and \cite{gu2020empirical} agree that the ability to uncover non-linearities and interactions is the crucial driving force of the superior ML performance.  As there are so many possible forms in which the predictors can interact, it is unfeasible to search for the correct specification manually. Neural Networks are universal approximators, and thus allow for systematic search in the space of non-linearities and interactions.  	
			
			Finally, the model selection is ML is based on validation data, as opposed to in-sample (training) data, and model evaluation done solely on held-out (testing) data. In traditional econometrics approach to stock returns, model is tuned by researcher based on in-sample fit, which results in unreplicable spurious findings \citep{mclean2016does}. On the other hand, ML puts an emphasis on out-of-sample performance, which allows to select a robust model that generalizes well to previously unseen data.  	
			
	
		\subsection{Interpretable ML and Stock Returns: What Has Been Done?}
		
			To explain how the methods employed in this thesis fit into the broader context of ML interpretability literature, it is useful to first categorize them following \cite{molnar2020interpretable}. A first dictinction can be made between \textit{intrinsically} interpretable models, which are intrepretable per se (linear and logistic regression, random trees), and \textit{post-hoc methods}, that can be applied to extract interpretable knowledge from more complex models, such as neural networks, which is the case of this thesis. A further distinction can be made as to the applicability of the interpretability methods: some methods are \textit{model-specific}, that is, can only be applied to some ML models, while others are \textit{model-agnostic}, applicable to all models from linear regressions to random trees and neural networks. For purposes of generality, the methods selected in this thesis are model-agnostic --- while they are well-suited to neural networks, they can also be used on any other model, such as linear regression. Yet other distinction can be made between \textit{global} and \textit{local} ML interpretability methods: while the former study how a model decides overall (across all observations), the latter can be used to interpret the drivers behind a single observation. Both approaches are taken in this thesis, as both global and local perspectives are important for model interpretability. 
			
			Further, there are several aspects of interpreting a neural network in particular. First, it is possible to study which input variables are important for the prediction (known as feature importance). Second, it is possible to see how the prediction changes in response to the input variables (marginal relationships). Finally, interaction effects and non-linearities can be uncovered from the hidden layers. This thesis takes the first two approaches, as it considers them natural starting points, and leaves the third for follow-up research. The three are now discussed in turn, in reverse order.  

			First, hidden layers can be analyzed to to uncover the feature interactions and patterns learned by the network. Hidden layer's values is a linear combination of the previous layer, with then applied non-linear function. From this perspective, the hidden neurons can be considered as features learned, or engineered, by the network. The hidden layers can be inspected ex post to answer the question of which \textit{interactions} between input features are important. For example, in the case of image recognition, it is possible to see that the network learns to recognize simple shapes (such as straight lines and circles) in the first hidden layers, and then proceeds to learn more intricate patterns (such as flowers and shapes of animals) in the deeper layers \citep{olah2017feature}. In the returns prediction task, it could be analogously studied which feature combinations are important. However, there is no such paper to the best of my knowledge. The closest to it is likely the interesting study of interaction terms in \cite{bryzgalova2019forest}, but it is a different model (random forest rather than neural network, i.e., no hidden layers) and a different prediction task (construction of basis assets rather than returns prediction). \cite{gu2020empirical} \textit{do} show interaction effects in a neural network predicting stock returns, but it is only an interaction of a single variable with 4 other variables (that is, 4 interactions out of 8649, and that is just counting the first-order), so these results are merely an illustration of an interesting area for future research. 
				
			Interestingly, Recurrent Neural Networks (RNNs), can be analyzed in similar manner to uncover the abstract patterns learned by the network. RNNs are used for modeling data with important time dimension, such as voice, text and financial time-series. \cite{di2016artificial} employ RNNs to predict stock price, but without focusing on interpretability. RNN interpretability is demonstrated in \cite{karpathy2015visualizing} on a language model: the authors are able to identify hidden cells that have interpretable meaning, such as neurons that "specialize" in recognizing passages in quotes or brackets. In the financial domain, \cite{giles1997rule} find that RNNs learn well-known patterns in exchange rate movements, such as momentum and reversal. While this thesis views stock returns prediction from the cross-sectional perspective, so these time-series methods are not applicable here, this literature offers promise.
			
			Second, it can be studied how the prediction changes in response to the input variables. To the best of my knowledge, there is a single attempt at this in the financial domain: \cite{gu2020empirical} plot the predicted return for different values of 4 selected predictors, each time holding the remaining 92 predictors at 0. In addition to the limited scope (only 4 variables out of 93 are studied), this has the obvious limitation of unrealistic datapoints: in reality, there are no observations with values of 0 for all but one feature; the correlation of data is also ignored. This thesis offers the marginal relationships of all features, calculated with theoretically well-grounded methodology. This allows to see the typical sign and magnitude of all the predictors in the network.
			
			Third, it can be studied which input variables are more important than others (global feature importance). Two papers are the closest to this thesis in this respect: \cite{gu2020empirical} and \cite{tobek2020does}. Both study the very same prediction task as this thesis: to predict the return of a stock in the next month, given the characteristics of the stock as of the current month. Also like this thesis, both use neural networks to model the relationship between characteristics and stock returns. And, like this thesis, both calculate the global importance. However, both focus on performance of the networks and admit that their "goal in interpreting machine learning models is modest" \cite[p.~2247]{gu2020empirical}. Specifically, the interpretation in both papers is reduced to a single global feature importance figure. This thesis builds on \cite{gu2020empirical} and \cite{tobek2020does} by diving fully into the global feature importance, while meeting their high performance. (The thesis uses the very same architecture of the networks as in \cite{gu2020empirical}, and dataset from \cite{tobek2020does}, so the high performance is to be credited wholly to their work.) The contributions in analysis of global feature importance over and above these two papers are several. First, this thesis chooses a feature importance metric that is well-grounded in theory and prior literature (\ref{chap:feature_importance_lit}). Second, it suggests and uses another, novel metric which is particularly suited for uncovering why neural networks are good at predicting winners and losers in the stock market. Third, it always compares the interpretation to linear regression, which points to the most important non-linearities. Fourth, it compares the interpretation across networks with different depth, which allows to see of the added complexity changes the model interpretation. Fourth, it interprets not only the ensembles, but their constituent models as well, which further unveils why the ensembles perform so well.     		
			
				
		\subsection{Choosing Feature Importance Measure}
			\label{chap:feature_importance_lit}
			Local feature importance measures aim to answer the question of why the model gives a particular prediction for a particular observation. For example, why does the model predict Apple's return in the following month to be 15\%? In particular, how do the individual features contribute in magnitude and sign to the prediction? In other words, how can the prediction be decomposed into (\textit{attributed to}) individual features? For this reason, the task is also called \textit{feature attribution}.  
			
			There are several properties that are naturally desirable for any attribution measure to have. First, we would like the measure to give a complete account of the prediction: if we sum the individual feature attributions, we would like to obtain the final prediction (called Completeness or Efficiency axiom in \cite{sundararajan2017axiomatic} and \cite{molnar2020interpretable} respectively). Second, the measure should give zero attribution score to any feature that is mathematically independent of the predicted variable (called Sensitivity(b) or Dummy axiom in \cite{sundararajan2017axiomatic} and \cite{molnar2020interpretable} respectively). A related desirable property is that if two observations differ in one feature and have different prediction, the attribution to the differing feature should be non-zero (called Sensitivity(a) axiom in \cite{sundararajan2017axiomatic}). Third, the measure should be additive: if a model is a linear combination of other models (such as an ensemble model), the attributions of the model should be the same linear combination of the attributions of the individual models (called Linearity or Additivity axiom in \cite{sundararajan2017axiomatic} and \cite{molnar2020interpretable} respectively). Fourth, if two models with different architectures produce the same predictions for the same set of inputs, the attributions of the two models should be identical (called Implementation Invariance axiom in \cite{sundararajan2017axiomatic}). As pointed out by \cite{sundararajan2017axiomatic}, it is particularly important in the case of neural networks that Implementation Invariance be upheld: a single functional form of the model can be attained using more than one set of weights due to many degrees of freedom. The training of the network can converge to any of these sets, depending, for instance, on random initialization of weights. However, if the function arrived at is effectively the same, it is desirable that the feature attribution be the same as well. Finally, symmetric features (i.e., such that $f(x,y) = f(y,x) \forall x, y$) should receive the same attributions if $x=y$ for a particular observation (called Symmetry axiom in \cite{shrikumar2017learning} and  \cite{molnar2020interpretable}. 
			
			Several local feature importance measures are put forward in the literature. \textbf{Simple Gradient Method} \citep{baehrens2010explain} applies a first order linear approximation of the model to find the sensitivity of the prediction to small changes in values of a given feature. It is a natural starting point for computing feature attribution, as it is very intuitive: if a small change in predictor's value has large effect on prediction, the gradient is large. Other approaches involve back-propagating the final prediction through all the layers down to the input layer. These include  \textbf{Guided Back-Propagation} \citep{springenberg2014striving}, \textbf{Layer-Wise Relevance Propagation} \citep{binder2016layer} and \textbf{DeepLift} \citep{shrikumar2017learning}. However, all these methods violate the most fundamental axioms, rendering the usage of the methods very problematic. Simple Gradient Method and Guided back-propagation violates Sensitivity(a) and Completeness and DeepLift and Layer-Wise Relevance violate Implementation Invariance \citep{shrikumar2017learning, sundararajan2017axiomatic}. The only measures used so far in the domain of stock-returns prediction (in \cite{gu2020empirical}, \cite{tobek2020does}) are variants of Simple Gradient Method, and thus suffer the same issues. 
			
			There are only two methods that satisfy the above axioms: Integrated Gradients \citep{sundararajan2017axiomatic} and Shapley (Shapley-Shubik) Values \citep{shapley1971assignment}. 	
			
			\textbf{ Integrated Gradients} \citep{sundararajan2017axiomatic} is a method similar to Simple Gradient Method in that it considers the gradient of the prediction with respect to the feature values. The gradient captures how the prediction changes for small changes in given feature. (Using the gradient is advantageous as it must be already calculated (with respect to weights) during network training.) The measure alters the Simple Gradient Method by considering the gradient not only exactly at the observation, but also at all points on the path from a baseline (usually $0$) to the observation at hand. This adjustment is already enough to achieve Completeness and Sensitivity(a) axioms that Simple Gradient Method breaks. 
			
			\textbf{Shapley Values} originate in game theory and their use in ML is intuitively explained in \cite{molnar2020interpretable}: for a single observation, imagine that the feature values enter a room in random order. At each point, all feature values present in the room make a prediction. Shapley value for feature $j$ is the average change in prediction that happens when feature $j$ enters the room. The average here can be taken either across all possible random orders as in \cite{shapley1971assignment} (there are $K!$ of them for $K$ features) or approximated using several samples as in \cite{vstrumbelj2014explaining}. 
			
			Actually, it can be shown \citep{sundararajan2017axiomatic} that the axioms of Completeness, Implementation Invariance, Sensitivity and Linearity are satisfied \textit{only} by a particular group of methods, called \textit{path methods}, which are all very much like Integrated Gradients, but may take other than straight-line path between the baseline and the observation at hand. Within path methods, Integrated Gradients can be shown \citep{sundararajan2017axiomatic} to be the only one that is symmetry preserving. Shapley Values are related to the path methods in that they average over \textit{multiple} paths, instead of just considering a single path. This is unfortunately the reason behind their computational intensity, as discussed below.  
			
			Integrated Gradients have one crucial practical advantage over Shapley Values. Even the approximate version of Shapley Value \citep{vstrumbelj2014explaining} is very computationally intensive: sampling a single random order of features then takes $K$ calls to the network --- a prediction is made \textit{each time} a feature "enters the room". So to calculate even the approximate Shapley Value for a single observation, the network must be called $K\cdot M$ times, where $M < K!$ is the number of steps in the approximation. On the other hand, the Integrated Gradients only take $M$ calls to the network. In case of many features and (or) many observations for which we wish to calculate local feature importance, and (or) long prediction times (note that all three are often the case in ML), the difference in computation times is crucial. Even though the number of features is only 30 in this thesis, the difference in computation times is already substantial. Moreover, much more features are to be expected in practice, so using a scalable measure seems vital. Therefore, this thesis uses Integrated Gradients as the feature importance measure. The technical aspects of calculating Integrated Gradients are described in the Data and Methodology. 
			
			The methods discussed so far are based on local interpretation, i.e., they allow to decompose the prediction into individual features. In addition to these local measures, there are \textit{global} feature importance measures, which do not allow for such decomposition, but can nonetheless offer insight into the overall workings of the model. In the domain of stock returns prediction, \cite{gu2020empirical} use \textbf{Decrease of R Square} ($R^2$, variance in the target variable explained by the model) that results from setting a particular feature's values to 0. This measure is problematic from two reasons. First, $R^2$ is notoriously unstable in returns prediction task: the metric fluctuates greatly due to the high ratio of noise to signal, which makes it even less informative than usual. Second, setting feature values to 0 completely denounces the marginal distribution of the feature and makes it flat, which is less realistic than the available alternatives \citep{fisher2019all}. \textbf{Model Reliance} proposed by \citep{fisher2019all} solves both these disadvantages. It measures how much of model's loss (here, it is the Mean Squared Error) is lost when a feature is permuted. This has the advantage of preserving the marginal distribution of the feature, while making it independent of the prediction. If the particular feature is important, permuting it will increase the model's loss. Model Reliance of the networks used in this thesis is given in the Appendix. Unfortunately, the Mean Squared Error of networks in the domain of stock return prediction is quite large and noisy, again due to the high ratio of noise to signal, which makes Model Reliance less useful than in other applications (see \cite{fisher2019all} and \cite{molnar2020interpretable}). This thesis uses instead a variant of Model Reliance: it replaces the Mean Squared Error by the decrease in return of the implied long-short portfolios (see \ref{chap:met}). This has the advantage of using a measure that is stable in the domain of predicting stock returns and of 		focusing on what the stock-predicting returns do best: predict winners and losers in the stock market. Using a corresponding metric allows to identify the sources of this out-performance. The metric, along with Model Reliance, is further described in Data and Methodology. 		

			
			
		
		 

	

