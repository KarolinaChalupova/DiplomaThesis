\chapter{Literature Review}
\label{chap:lit} 
		
 	The question of what drives the stock returns is a thrilling one to answer from several perspectives: that of an economist, of a statistician, of a machine learning engineer, and that of a finance practitioner.
 	An economist sees that stock returns reflect human decisions: putting a price tag on uncertain future payoffs reflects the trade-off between current and future consumption, human impatience and attitudes to risk \citep{cochrane2009asset} as well as human behavioral biases [TODO add reference]. She also sees that they speak of the complex web of relations between firms: as companies form relationships, they become exposed to similar risks, and their returns correlate. For the economist, studying the determinants of stock returns therefore is an opportunity to understand human behavior as well as the macro-phenomena that emerge in the network of firms' relationships. 
 	The statistician sees that the determinants of stock returns are notoriously over-studied, which brings many issues of publication bias and multiple hypothesis testing, leading to many false discoveries \citep{harvey2016and}. From this perspective, the question stands: how to separate the wheat from the chaff? 
 	The machine learning engineer sees the problem as a prediction task: there is a large amount of data, the variables likely interact in complex ways, and are highly correlated. Machines have proven to excel in the task of predicting stock returns. They learn from financial data, model the interactions, and produce unparalleled returns predictions \citep{gu2020empirical}.
 	Finally, for the finance practitioner seeks to identify the future winners and losers, and sell and purchase them to make profit. As the machine-learning approaches are becoming the norm in the applied field, a need arises to understand the models on a deeper level â€“ to interpret them, which brings us back to the economic underpinnings of stock returns.
 	
 	This thesis attempts to bring the four approaches together. I use machine learning models to predict stock returns and then interpret them to bring us closer to the answers to the economists' questions. This approach can also help to filter out false discoveries. Finally, the finance practitioner can employ ML interpretability methods to find an intelligible model, of which she understands the weaknesses and sources of performance. 
 	
 	The literature review proceeds as follows. First, I review the economic theory behind asset prices and common attitudes to modeling them. Second, I present the statistician's view: the multidimensional challenge faced in modeling stock returns \citep{cochrane2011presidential} and show what different approaches have been used to answer the challenge. Third, I present the machine learning approaches to asset pricing. Fourth, I review ML interpretability methods. 
 	
 	\section{Economic Theory Behind Stock Returns Prediction}
 	
	 	Valuing a stock amounts to putting a price tag on a stream of future payoffs. The field concerned with doing just that is called asset pricing. Consider the following basic asset-pricing equation \citep{cochrane2009asset}:
	 	
	 	\begin{equation}
	 		p_t^i = E_t(m_{t+1} x_{t+1}^i ) \label{eq:pEmx}
	 	\end{equation}
	 	
	 	In words, the price of an asset $i$ at time $t$ ($p_t^i$) is proportional to the expected asset's payoff at time $t+1$ ($x_{t+1}^i$). However, since the payoff does not come now ($t$), but in future ($t+1$), we discount it to present by the factor $m_{t+1}$, called the \textit{stochastic discount factor}. The term stochastic stands to express the idea that $m_{t+1}$ is not known with certainty at time $t$. There is only one assumption made in order to write the equation: it can be shown that $m_{t+1}$ exists if and only if the \textit{law of one price} holds, that is, if two assets generate the exact same payoff in all possible states of nature have the same price. If this is the case, there is a discount factor such that the equation holds for all assets $i$ \citep{cochrane2009asset}.
	 	
	 	In the stock market, equation \ref{eq:pEmx} translates to:
	 	
	 	\begin{equation}
	 		1 = E_t(m_{t+1} R_{t+1}^i ) \label{eq:1EmR}
	 	\end{equation} 
	 	
	 	In words, an investor pays 1 dollar now to collect $R_{t+1}^i$ dollars in the future. The payoff is called gross return and is the sum of future price and dividend $R_{t+1}^i = p_{t+1}^i+ d_{t+1}^i$. 
	 	Using the definition of covariance, we can rearrange \ref{eq:1EmR} \citep{cochrane2009asset}:
	 	
	 	\begin{equation}
	 		1 = E_t(m_{t+1}) E_t(R_{t+1}^i) + Cov_t(m_{t+1},R_{t+1}^i) 
	 	\end{equation} 
 	
 		Defining the \textit{risk-free gross return} as $R_{t+1}^f=\frac{1}{E_t(m_{t+1})}$ and using it to further rearrange \citep{cochrane2009asset}:
 		
 		\begin{equation}
 			E_t(R_{t+1}^i) = \underbrace{R_{t+1}^f}_\text{risk-free return}  -  \underbrace{R_{t+1}^f Cov_t(m_{t+1},R_{t+1}^i)}_\text{risk adjustment} \label{eq:risk_adjustment}
 		\end{equation} 
 		
 		This important result shows that expected return if stock $i$ can be decomposed into risk-free return and risk adjustment. Note that the risk adjustment occurs if and only if returns are correlated to the discount factor, so \textit{idiosyncratic} risk, that is, uncorrelated with the discount factor, is uncompensated. Returns positively correlated to the discount factor should be low, and vice versa. So to explain average returns, we "only" need to explain the returns' correlation to the discount factor $m_{t+1}$. 
 		
 		Further rearranging \ref{eq:risk_adjustment} provides one more insight \citep{cochrane2009asset}. Multiply both sides by $Var_t(m_{t+1})/Var_t(m_{t+1})$ to obtain:
 		
 		\begin{align}
	 		E_t(R_{t+1}^i) 
	 		& = {R_{t+1}^f} + 
	 		\underbrace{
	 			\frac{Cov_t(m_{t+1},R_{t+1}^i)}{Var_t(m_{t+1})}
	 			}_\text{denote $\beta_{i,t}$} 
 			\cdot 
 			\underbrace{
 				\left(-\frac{Var_t(m_{t+1})}{E_t(m_{t+1})}\right)
 				}_\text{denote $\lambda_{t}$} \label{eq:factor_model} \\
 			& = \underbrace{{R_{t+1}^f}
 				}_\text{risk-free return} 
 				+ 
 				\underbrace{\beta_{i,t} \cdot \lambda_{t}
 				}_\text{risk adjustment} \label{eq:beta_reprezentation}
 		\end{align} 
 		
 		This equation is so-called \textit{beta-representation} of \ref{eq:1EmR}. It shows that the risk-adjustment, the premium a stock pays for being correlated with the discount factor, can be decomposed into $\beta_{i,t}$ and $\lambda_t$. $\lambda_t$ is the volatility of the discount factor and it is unrelated to properties of asset $i$. It can be interpreted as the price of risk. $\beta_{i,t}$ can be interpreted as the amount of the risk inherent in asset $i$. Empirically, it can be obtained as the coefficient from regressing asset $i$'s returns on the discount factor [TODO does this estimation introduce the assumption that beta is time-invariant?]: 
 		
 		\begin{equation}
 		R_{t+1}^i = a_i + \beta_{i}m_{t+1} + \epsilon_{i,t+1}
 		\end{equation} 
 		
 		Equations \ref{eq:risk_adjustment} and \ref{eq:beta_reprezentation} show that the discount factor is the key to explaining stock returns, as the latter is nothing but compensation for correlation with the former. Thus, specifying the discount factor is the only content of any asset-pricing model \cite{cochrane2009asset}. I review the consumption-based model, the CAPM, and multiple-factor models as special cases of these equations providing more intuition about what drives the discount factor. 
 			 	
	 	\subsection{Consumption-Based Model}
	 	
		 	The consumption-based model takes the additional assumption that the investor's preferences can be captured by her utility from consumption. The investor needs to decide how much to consume today and how much to save for tomorrow. First order conditions for this problem lead to the following specification of the discount factor \cite{cochrane2009asset}: 
		 	
		 	\begin{equation}
		 		m_{t+1} = \kappa \frac{u'(c_{t+1})}{u'(c_t)} \label{consumtion_based_model}
		 	\end{equation}
		 	
		 	where $u'(c_{t+1})$ denotes marginal utility from consumption. That is, the discount factor is the consumer's  marginal rate of substitution between consumption at time $t$ and $t+1$ and captures the willingness to trade consumption today for consumption tomorrow, where parameter $\kappa$ is the weight placed on future utility. This means that such an investor would demand a high return for stock that perform badly at times when she is unwilling to give up today's consumption, and a low return for stock that performs badly at times she is willing to give up today's consumption. 
		 	
		 	For simplification, one can assume constant relative risk aversion:
		 	
		 	\begin{equation}
		 		u(c_t) = \frac{c^{1-\gamma}-1}{1-\gamma}
		 	\end{equation}
		 	
		 	where $\gamma$ is a parameter positive for risk-averse individuals, and plug in to \ref{consumtion_based_model}: 
		 	
		 	\begin{equation}
		 	m_{t+1} = \kappa \left(\frac{c_{t+1}}{c_t}\right)^{-\gamma} 
		 	\end{equation}
		 	
		 	 That is, the discount factor is inversely proportionate to consumption growth. \ref{eq:risk_adjustment} says that assets with positive correlation with the discount factor should have lower returns, so it follows that assets highly correlated with consumption growth should earn high returns. This makes sense: the investor is assumed to care only about her marginal utility from consumption. Therefore, she demands a higher return for holding a stock that performs badly at times when her consumption decreases, and only a low return on insurance-like stocks that perform well during bad times.
		 	 
		 	 For the sake of completeness, we can rewrite this to the equivalent beta-representation using \ref{eq:beta_reprezentation} [TODO odvodit, overit, zpresnit notaci]: 
		 	 
		 	 \begin{align}
		 	 & E_t(R_{t+1}^i) = R_{t+1}^f + \beta_{i,t} \cdot \lambda_{t} \\
		 	 & \lambda_{t} = \gamma Var_t(\Delta c_{t+1}) \\
		 	 & R_{t+1}^i = \alpha_{i,t} + \beta_{i,t} \Delta c_{t+1} + \epsilon_{i,t+1}
		 	 \end{align}
		 	 
		 	  However, this model does not perform very well empirically \cite{cochrane1996cross}. There are two possible reasons, either, the aggregate consumption data are imperfectly measured, or the assumption that the investor maximizes the utility from consumption is off. This motivates tying the discount factor to other variables.
	 	
	 	\subsection{Capital Asset Pricing Model}
	 	
	 		The Capital Asset Pricing Model (CAPM) is the possibly most widely-known asset pricing model. It is the special case of the consumption-based model, making the additional assumption that the utility from consumption is logarithmic \citep{rubinstein1976valuation}:
	 		
	 		\begin{equation}
	 			u(c_t) = ln(c_t)
	 		\end{equation}
	 		
	 		Plugging this to \ref{consumtion_based_model}, one obtains 
	 		
	 		\begin{equation}
	 			m_{t+1} = \kappa \frac{c_t}{c_{t+j}}
	 		\end{equation}
	 		
	 		The CAPM operates with the concept of wealth portfolio, which comprises all world's wealth, inclusing real estate, metals, machinery or art. The price of this portfolio is (by \ref{eq:pEmx} and plugging in for the discount factor):  
	 		
	 		\begin{equation}
	 		p_t^W  = E_t \sum_{j=1}^{\infty} m_{t+j}c_{t+1} = \frac{\kappa}{1-\kappa}c_t
	 		\end{equation}
	 		
	 		and the return on wealth portfolio is therefore 
	 		
	 		\begin{equation}
	 		R_{t+1}^W  = \frac{p_{t+1}^W + c_{t+1}}{p_t^W} = \frac{1}{\kappa} \frac{c_{t+1}}{c_t} = \frac{1}{m_{t+1}}
	 		\end{equation}
	 		
	 		Thus, according to CAPM, the discount factor is the inverse of the return on the wealth portfolio. As \ref{eq:risk_adjustment} says that assets with positive correlation with the discount factor should earn low returns, if follows that assets positively correlated with the wealth portfolio should earn higher returns. Again, this makes sense: an investor who only cares about consumption demands a higher return for stocks that perform badly when other sources of wealth perform badly, and vice versa, she is willing to forgo some average return in exchange for good performance at times when all else fails. 
	 		
	 		Again, for completeness, we can rewrite this to the equivalent beta-representation using \ref{eq:beta_reprezentation} [TODO odvodit, overit, zpresnit notaci]: 
	 		
	 		\begin{align}
	 			& E(R_{t+1}^i) = \gamma + \beta_{i,t} \cdot \lambda_{t} \\
	 			& \lambda_{t} = E_t(R^W_{t+1}) - \gamma \\
	 			& R_{t+1}^i = \alpha_{i,t} + \beta_{i,t} R^W_{t+1} + \epsilon_{i,t+1}
	 		\end{align}  
	 		
	 		CAPM accomplishes the task of removing consumption data from empirical estimation of discount factor \citep{cochrane2009asset}, replacing them by return on wealth portfolio. However, this return is unobservable, so it is empirically often replaced by return on market portfolio of stocks. This may be problematic, as wealth comprises many more assets than just stocks, so the approximation is likely too crude \citep{roll1977critique}.  
	 		
	 		
	 	\subsection{Multi-factor Models}
	 		
	 		Multi-factor models are motivated by the notions that consumption growth and return on wealth portfolio do not proxy the discount factor empirically very well (in the consumption-based model and CAPM respectively), and they try to find other proxies for the discount factor $m_{t+1}$, factors $\vec{f}_{t+1}$. If we retain the idea that investor maximizes her utility from consumption, then the factors  should proxy the marginal utility growth \citep{cochrane2009asset}: 
	 		
	 		\begin{equation}
	 		m_{t+1} = \kappa \frac{u'(c_{t+1})}{u'(c_t)} \approx a+\vec{b}'\vec{f}_{t+1}
	 		\end{equation}
	 		
	 		The expression on the left-hand side can be interpreted as the rate at which the investor is willing to swap future for current consumption. [TODO verify that this understanding of mine is correct]. There are many variables that affect this rate. The current state of the economy, such as GDP growth, interest rate or investment, as well as new of the future states, forecasting either asset returns or macroeconomic variables. Many candidate factors can be defended on these grounds.   
	 		
	 		Multi-factor models explain returns as	 
	 	
	 	
			 	\begin{align}
			 		& E_t(R^i_{t+1}) = R^f_{t+1} + \vec{\beta}'_{i,t} \vec{\lambda}_{t} \\
			 		& R^i_{t+1} = \alpha_{i,t} + \vec{\beta}'_{i,t} \vec{f}_{t+1} + \epsilon_{i,t+1} \label{multifactor model}
			 	\end{align}
			 
			 The \textit{factor loadings} $\vec{\beta}'_{i,t}$ of size $1\times K$ are exposures to risk factors $\vec{f}_{t+1}$ of size $K \times 1$, where $K$ is the number of risk factors. They represent the amount of risk inherent in asset $i$ due to its correlation to the corresponding risk factor. The $\vec{\lambda}_{t}$ of size $K \times 1$ are interpretable as the risk prices at time $t$ \citep{kelly2019characteristics}. (Note that the CAPM and consumption-based models are special cases of the multiple-factor models, where $K$, the number of factors, is 1.) 			
			 
			 First multi-factor model is \cite{fama1996multifactor} [TODO make sure the time indexation is right]:
			 
			 \begin{align}
			 	& E(R^i) = R^f + \vec{\beta}'_{i} \vec{\lambda} \\
			 	& R^i_{t+1} = \alpha_{i} + \vec{\beta}'_{i} \vec{f}_{t+1} + \epsilon_{i,t+1}
			 \end{align}
		 
		 	with factors  
			  		
				\begin{equation}
					\vec{f}_{t+1} = 
						\begin{pmatrix}
							R^M_{t+1}-R^f_{t+1} \\
							SMB_{t+1} \\
							HML_{t+1}				
						\end{pmatrix}
				\end{equation}
			
			and with factor prices 
			
			\begin{equation}
			  \vec{\lambda} =
				\begin{pmatrix}
					E(R^M)-R^f \\
					E(SMB) \\
					E(HML)				
				\end{pmatrix} 
			\end{equation}
			 
			The $R^M_{t+1}-R^f_{t+1}$ is the return on market portfolio above risk-free rate (as in CAPM), $SMB_{t+1}$ is the return on small-cap portfolio minus the return on big-cap portfolio, and $HML_{t+1}$ is the return on portfolio of stocks with small market values relative to book value, minus the return on portfolio of stocks with high market values relative to book value. Note that the risk prices $ \vec{\lambda}$ and $\vec{\beta}'_{i}$ are time-invariant. Explanations why $SMB$ and $HML$ should be correlated to the discount factor include that they proxy for financial distress \citep{fama1996multifactor}, \citep{heaton2000portfolio} or that they forecast the future state of the economy \citep{liew2000can}. 		
			\cite{fama2015five} augment this three-factor model to five factors, adding $RMW$ and $CMA$, which are, respectively, the difference in returns on portfolios of firms with robust and weak profitability, and difference in returns on portfolios of firms with conservative and aggressive investment. 
			
			[TODO add a systematic overview of existing anomalies.]
			
			[TODO describe portfolio sorting metodology]
			
			It is difficult to empirically estimate a multifactor model. The main challenge is that both $\vec{f}_{t+1}$ and $\vec{\beta}'_{i,t}$ are unobserved, so we have no directly observed variables at the right hand side of the estimated equation. There are two different approaches to tackle this issue \citep{kelly2019characteristics}. The first is to use use prior knowledge of empirical behavior of average returns to pre-specify the factors, treat them as observable and then estimate $\beta_{t-1}$. An example of this approach is \citep{fama1993common} and most of the hundreds of published factors are too \cite{cochrane2009asset}. But as noted by \cite[p.~3]{kelly2019characteristics}, this prior specification relies on "a partial understanding at best, and at worst is exactly the object of empirical interest."  Even \cite{fama1993common} note that without a clear economic theory for factors, the choice of the right-hands side variables is arbitrary. But as we have seen, the economic theory justifies inclusion of almost anything as the right-hands side variable: "One can appeal to the APT or ICAPM to justify the inclusion of just about any desirable factor" \citep[p.~124]{cochrane2009asset}), so we do not have a clear economic theory.  
			
			The other way to uncover the factor structure is, therefore, to instrument the unobserved factor loadings using firm-level variables, commonly called \textit{characteristics}.  
			
			\begin{align}
				E_t(R^i_{t+1}) = R^f_{t+1} + \vec{\beta}'_{i,t} \vec{\lambda}_{t}  \label{ER_betalambda} \\
				R^i_{t+1} = \alpha_{i,t} + \vec{\beta}'_{i,t} \vec{f}_{t+1} + \epsilon_{i,t+1} \\
				\vec{\beta}'_{i,t} = g(\vec{z}'_{i,t}) \label{instrumented_betas}
			\end{align}
			
			The first two equations are the same as in multiple-factor models, while the third shows instrumenting of the factor loadings with firm characteristics $\vec{z}'_{i,t}$. This approach is exemplified by \cite{kelly2019characteristics}. The key insight is that characteristics serve as proxies for \textit{exposure} to different sources of systematic risk. Once risk loadings are instrumented, the authors use them to estimate the corresponding factors.
	
	
	\section{Which Predictors Matter Empirically?} 
		
		\epigraph{We have a lot of questions to answer: First, which characteristics really provide independent information about average returns?}{\cite{cochrane2011presidential}}
		
		There are hundreds of published factors suggested as explanation of average stock returns. \cite{harvey2016and} count 313 variables, considering the top journals only and calling the count "surely too low". Indeed, \cite{cochrane2011presidential} refers to the state of asset pricing as a "zoo of factors". At the same time, 
		the number of factors should be low in theory \citep{cochrane2011presidential} and empirically \citep{ahn2012determining}.
		
		The task of establishing which variables predict stock returns has proved treacherous to many researchers.  As \cite[p.~5]{harvey2016and} put it rather famously: "most claimed research findings in financial economics are likely false". And by financial economics, the authors mean explaining the cross-section of stock returns. Why is this? The main issue is that most published predictors of stock returns are found only controlling for 3 or 5 factors, typically factors from  \cite{fama1996multifactor} or \cite{fama2015five}, and not the rest of the published factors. But there are more issues. First and most obviously, some results are bound to be just false positives due to the typical confidence levels \citep{harvey2016and}. Second, as it is difficult to publish a non-result, the insignificant results remain in the drawer and the significant ones get published, artificially driving up the significance in an instance of publication bias \citep{harvey2016and}. Third, the published studies are often biased in themselves: the specification search bias (selection of the model based on model's result), the sample selection bias (selection of the data based on model's results) and the multiple hypothesis testing bias (conducting multiple tests of the same hypothesis) all result in artificially high significance of returns predictors \citep{mclean2016does}. The situation is worsened by the fact that there is only a limited amount of data in finance: CRSP and Compustat are limited resources, and researches use the same data over and over, resulting in collective over-fitting \citep{harvey2016and}. Finally, unlike in other fields, it is difficult to publish a replication study in finance, which leaves the spurious factors unnoticed \citep{harvey2016and}. 
		
		Two post-hoc approaches have been taken to find out which of these findings are false: out-of-sample tests and rising the usual significance levels. \cite{mclean2016does} perform the out-of-sample tests, studying 82 variables. As much as 10 of them could not even be replicated in-sample. The rest are biased on average by 10\%. \cite{harvey2016and} take the multiple-testing approach. Out of 296 published significant factors, 158, 142, 132 and 80 are false discoveries, the precise number depending on statistical framework used \cite{harvey2016and}. 
		
		\cite[p.~1060]{cochrane2011presidential} formulates the problem of separating the wheat from the chaff in stock returns prediction in his "multidimensional challenge": "We have a lot of questions to answer: First, which characteristics really provide independent information about average returns? Why are subsumed by others? Second, does each new anomaly variable also correspond to a new factor formed on those same anomalies? (\ldots) Third, how many of these new factors are really important?" 
		
		It is important to understand the distinction between characteristics and factors. Both are used as predictors in stock returns, although never in a single regression.  Characteristics are firm-specific variables, such as firm size, returns momentum or book-to-market ratio. The economic motivation behind using firm characteristics to predict stock returns is that they are proxies for underlying sources of risk. \cite{bryzgalova2019forest, kelly2019characteristics, gu2020empirical}. Thus, when studying the cross-section of returns using characteristics as predictors, all observations across time and across firms enter into a single regression. This approach is exemplified by \cite{kelly2019characteristics, gu2020empirical, bryzgalova2019forest}. Factors, on the other hand, are the underlying sources of risk themselves, common to all firms. An example of a factor would be the market return. When modeling the cross-section of returns using factors, each firm enters its own regression, where the predictors are the common factors and the estimated weights are the sensitivities of the firm's return on that factor, such as market beta. A classical example is \cite{fama1996multifactor, fama2015five}. Note that the sensitivites in the factor model can be used as predictors in the characteristics model, which is how the two approaches relate to each other. 
		
		Current asset pricing literature is still looking for the solution to the first of Cochrane's challenges: which characteristics inform the prediction of stock returns. The most current answer that the literature offers is as follows. 
		
		The characteristics 
		
		\cite{gu2020empirical} study 94 characteristics proposed by prior literature in various settings, starting with OLS without penalty, across penalized OLS, to random trees and neural networks. The authors find that all methods (with the exception of OLS) agree in the variables that have an important first-order impact on returns. These are, ordered by importance: recent price trends (such as momentum and long- and short-term reversal), liquidity measures (such as turnover), risk measures (such as return volatility and market beta), valuation ratios (such as earnings-to-price), and fundamental signals (such as asset growth). On annual frequency, recent price trends become less important, and industry emerges as an important predictor, but otherwise the results are similar to the monthly frequency. 
		
		\cite{bryzgalova2019forest} study recent price trends (momentum and long- and short-term reversal), liquidity measures (such as turnover), investment, profitability, accruals and book-to-market ratio. [TODO find important, compare to Gu]. 		
		
				
		\section{Why use ML for Stock Returns Prediction?}
				
			
			One of the reasons behind popularity of ML in stock returns prediction is that it offers superior performance. \cite{gu2020empirical} make a horse race of several ML and traditional methods. First, they include methods to estimate linear $g$, both without regularization (linear regression with all variables (OLS), linear regression only with size, book-to market, and mometum (OLS-3)) and with regularization (partial least squares (PLS), principal components regression (PCR), elastic net (ENet)). Second, they include methods to estimate nonlinear $g$: generalized linear models (GLM), random forest, gradient-boosted regression trees, and neural networks of depths 1 to 5. They find that OLS performs badly, regularized linear models are an improvement, and, and nonlinear models are the best-performing, namely neural networks followed by random trees. 
			
			Second, there is strong theoretical appeal to using ML as well. Additionally, prior research shows that linear models miss important information about stock returns by ignoring the interactions of the predictors. If a variable has a non-linear relationship with returns, the linear models can conclude there is no association, while in reality there is an important non-linear relationship. For example, in \cite{gu2020empirical}, linear models deem size and volatility unimportant predictors, while non-linear models find the contrary. A similar problem may arise if predictors are have zero association with return themselves, but not in interaction with other predictors. The nonlinearity seems to be a curcial aspect of stock return prediction: \cite{bryzgalova2019forest} and \cite{gu2020empirical} agree that the ability to uncover non-linearities and interactions is the crucial driving force of the superior ML performance and \cite{gu2020empirical} statistically reject linear models in favor of the non-linear ones as those with better performance. As there are so many possible forms in which the predictors can interact, it is unfeasible to search for the correct specification manually. ML offers methods of choosing the right functional form systematically. 
			
			Moreover, the model selection is ML is based on validation data, as opposed to in-sample (training) data. In traditional econometrics approach to stock returns, model is tuned by researcher based on in-sample fit, which results in spurious findings that are very often not replicable out-of-sample \citep{mclean2016does}. On the other hand, ML puts an emphasis on out-of-sample performance, which allows to select a robust model. 	
			
			Finally, ML methods are designed to handle data with high dimensionality and correlation in predictors. ML offers dimension reduction and variable selection methods to overcome this challenge, and thus presents a natural way of approaching Cochrane's multidimensional challenge \citep{cochrane2011presidential}. 	
			
	
	\section{Interpretable Machine Learning Methods}
	
		To explain how the methods employed in this thesis fit into the broader context of ML interpretability literature, it is useful to first categorize them following \cite{molnar2020interpretable}. A first dictinction can be made between \textit{intrinsically} interpretable models, which are intrepretable per se (linear and logistic regression, random trees), and \textit{post-hoc methods}, that can be applied to extract interpretable knowledge from more complex models, such as neural networks, which is the case of this thesis. A further distinction can be made as to the applicability of the interpretability methods: some methods are \textit{model-specific}, that is, can only be applied to some ML models, while others are \textit{model-agnostic}, applicable to all models from linear regressions to random trees and neural networks. For purposes of generality, the methods selected in this thesis are model-agnostic --- while they are well-suited to neural networks, they can also be used on any other model, such as linear regression. Yet other distinction can be made between \textit{global} and \textit{local} ML interpretability methods: while the former study how a model decides overall (across all observations), the latter can be used to interpret the drivers behind a single observation. Both approaches are taken in this thesis, as both global and local perspectives are important for model interpretability.
			
			
		\subsection{Interpretable Machine Learning Methods and Stock Returns: What Has Been Done?}
			\cite{bryzgalova2019forest} use an intrinsically interpretable model to study stock returns. Rather than predicting stock returns as do \cite{gu2020empirical} and this thesis, the ML model is used here to build the discount factor (see the section on Asset Pricing Theory for what is the discount factor). The model used is random tree. A random tree iteratively splits data into two parts, using a particular feature and its particular value to make each split. It starts with the entire dataset, performs the first split in two parts, the two parts are then split in two again etc. recursively, until a stopping criterion is met. The latest, unsplit groups of the dataset, called leaves, are then groups of the data that share the same prediction. \cite{bryzgalova2019forest} note that this is a generalization of the portfolio sorts, which is a method typically used in finance to arrive at the discount factor. The leaves of the tree are used as basis assets for forming the discount factor. Since the splits are not made by the researcher, but instead learned by the model itself, this presents a viable approach to forming discount factor from multi-dimensional data. This overcomes the issue with portfolio sorts, which break down when the dimensionality of the data, i.e., number of features, is large \citep{cochrane2011presidential, bryzgalova2019forest). The advantage of random trees is that they are quite easily interpretable: to understand how prediction is made for a single feature, it suffices to follow the path the feature takes through the tree from the root to the leaves, always classified into group A or B based on a single feature. 
			
			Random trees are also used for returns prediction. In this case, the resulting leaves are not used to form basis assets as in \cite{bryzgalova2019forest}, but the firms inside a leaf share the return prediction. [TODO find a paper, there must be something.]
			
			\cite{gu2020empirical} 
			
			
			
				
		\subsection{Choosing Local Feature Importance Measure}
			Local feature importance measures aim to answer the question of why the model gives a particular prediction for a particular observation. For example, why does the model predict Apple's return in the following month to be 15\%? In particular, how do the individual features contribute in magnitude and sign to the prediction? In other words, how can the prediction be decomposed into (\textit{attributed to}) individual features? For this reason, the task is also called \textit{feature attribution}.  
			
			There are several properties that are naturally desirable for any attribution measure to have. First, we would like the measure to give a complete account of the prediction: if we sum the individual feature attributions, we would like to obtain the final prediction (called Completeness or Efficiency axiom in \cite{sundararajan2017axiomatic} and \cite{molnar2020interpretable} respectively). Second, the measure should give zero attribution score to any feature that is mathematically independent of the predicted variable (called Sensitivity(b) or Dummy axiom in \cite{sundararajan2017axiomatic} and \cite{molnar2020interpretable} respectively). A related desirable property is that if two observations differ in one feature and have different prediction, the attribution to the differing feature should be non-zero (called Sensitivity(a) axiom in \cite{sundararajan2017axiomatic}). Third, the measure should be additive: if a model is a linear combination of other models (such as an ensemble model), the attributions of the model should be the same linear combination of the attributions of the individual models (called Linearity or Additivity axiom in \cite{sundararajan2017axiomatic} and \cite{molnar2020interpretable} respectively). Fourth, if two models with different architectures produce the same predictions for the same set of inputs, the attributions of the two models should be identical (called Implementation Invariance axiom in \cite{sundararajan2017axiomatic}). As pointed out by \cite{sundararajan2017axiomatic}, it is particularly important in the case of neural networks that Implementation Invariance be upheld: a single functional form of the model can be attained using more than one set of weights due to many degrees of freedom. The training of the network can converge to any of these sets, depending, for instance, on random initialization of weights. However, if the function arrived at is effectively the same, it is desirable that the feature attribution be the same as well. Finally, symmetric features (i.e., such that $f(x,y) = f(y,x) \forall x, y$) should receive the same attributions if $x=y$ for a particular observation (called Symmetry axiom in \cite{shrikumar2017learning} and  \cite{molnar2020interpretable}. 
			
			Several local feature importance measures are put forward in the literature. \textbf{Simple Gradient Method} \citep{baehrens2010explain} applies a first order linear approximation of the model to find the sensitivity of the prediction to small changes in values of a given feature. It is a natural starting point for computing feature attribution, as it is very intuitive: if a small change in predictor's value has large effect on prediction, the gradient is large. Other approaches involve back-propagating the final prediction through all the layers down to the input layer. These include  \textbf{Guided Back-Propagation} \citep{springenberg2014striving}, \textbf{Layer-Wise Relevance Propagation} \citep{binder2016layer} and \textbf{DeepLift} \citep{shrikumar2017learning}. However, all these methods violate the most fundamental axioms, rendering the usage of the methods very problematic. Simple Gradient Method and Guided back-propagation violates Sensitivity(a) and Completeness and DeepLift and Layer-Wise Relevance violate Implementation Invariance \citep{shrikumar2017learning, sundararajan2017axiomatic}.
			
			There are only two methods that satisfy the above axioms: Integrated Gradients \citep{sundararajan2017axiomatic} and Shapley (Shapley-Shubik) Values \citep{shapley1971assignment}. 	
			
			\textbf{ Integrated Gradients} \citep{sundararajan2017axiomatic} is a method similar to Simple Gradient Method in that it considers the gradient of the prediction with respect to the feature values. The gradient captures how the prediction changes for small changes in given feature. (Using the gradient is advantageous as it must be already calculated (with respect to weights) during network training.) The measure alters the Simple Gradient Method by considering the gradient not only exactly at the observation, but also at all points on the path from a baseline (usually $0$) to the observation at hand. This adjustment is already enough to achieve Completeness and Sensitivity(a) axioms that Simple Gradient Method breaks. 
			
			\textbf{Shapley Values} originate in game theory and their use in ML is intuitively explained in \cite{molnar2020interpretable}: for a single observation, imagine that the feature values enter a room in random order. At each point, all feature values present in the room make a prediction. Shapley value for feature $j$ is the average change in prediction that happens when feature $j$ enters the room. The average here can be taken either across all possible random orders as in \cite{shapley1971assignment} (there are $K!$ of them for $K$ features) or approximated using several samples as in \cite{vstrumbelj2014explaining}. 
			
			Actually, it can be shown \citep{sundararajan2017axiomatic} that the axioms of Completeness, Implementation Invariance, Sensitivity and Linearity are satisfied \textit{only} by a particular group of methods, called \textit{path methods}, which are all very much like Integrated Gradients, but may take other than straight-line path between the baseline and the observation at hand. Within path methods, Integrated Gradients can be shown \citep{sundararajan2017axiomatic} to be the only one that is symmetry preserving. Shapley Values are related to the path methods in that they average over \textit{multiple} paths, instead of just considering a single path. This is unfortunately the reason behind their computational intensity, as discussed below.  
			
			Integrated Gradients have one crucial practical advantage over Shapley Values. Even the approximate version of Shapley Value \citep{vstrumbelj2014explaining} is very computationally intensive: sampling a single random order of features then takes $K$ calls to the network --- a prediction is made \textit{each time} a feature "enters the room". So to calculate even the approximate Shapley Value for a single observation, the network must be called $K\cdot M$ times, where $M < K!$ is the number of steps in the approximation. On the other hand, the Integrated Gradients only take $M$ calls to the network. In case of many features and (or) many observations for which we wish to calculate local feature importance, and (or) long prediction times (note that all three are often the case in ML), the difference in computation times is crucial. Even though the number of features is only 30 in this thesis, the difference in computation times is already substantial. Moreover, much more features are to be expected in practice, so using a scalable measure seems vital. Therefore, this thesis uses Integrated Gradients as the local feature importance measure. The technical aspects of calculating Integrated Gradients are described in the Data and Methodology. 
		
		
		\subsection{Global Feature Importance Measure} 
		
			\paragraph{Mean Decreased Performance} proposed by \citep{fisher2019all}, also known as \textbf{Permutation Feature Importance} is a model-agnostic measure of feature importance, which measures how much of model's performance is lost when permuting a given feature. It compares the performance of a model fit on a set of features with the performance of the same model after permuting a given feature (randomly shuffling its features across examples, without re-fitting the model). If the particular feature is important, permuting it will decrease model's performance. There are several special cases depending on the measure the model's performance, e.g., \textbf{Model Reliance} \citep{fisher2019all} (mean squared error), or \textbf{Mean Decreased Accuracy} [TODO add citation] (accuracy). Since the measure depends on random permutation, its variance can be decreased by computing it several times and averaging the result. [TODO add alogorhitmic description.]	
			
			\paragraph{Mean Decreased Explained Variance} is also a model-agnostic measure of feature importance, and measures how much of model's explained variance is lost when replacing the feature's values.  It is different to Mean Increased Loss in that rather than measuring changes in model's \textit{error}, it measures the decrease in explained variance of the target, or $R^2$. It is used for example by \cite{gu2020empirical} or \cite{kelly2019characteristics}. [TODO find original methodological paper introducting the measure].  \cite{gu2020empirical} use replacement of values by 0, as their features are scaled from $-1$ to $1$, but generally a different replacement is needed [TODO could be permutation? I think so. In that case average needs to be taken.]
			
			\paragraph{Sum of Squared Derivatives} proposed by \cite{dimopoulos1995use} is the sum of squared partial derivatives of the model's prediction with respect to the given feature. The disadvantage of this measure that is its not model-agnostic, specifically it is unusable if derivatives cannot be taken such as in random trees. 
			
			\paragraph{Mean Decreased Impurity} proposed by [TODO find out] can be used for tree-based methods, unlike Sum of Squared Derivatives.  
			
		
		\subsection{Other Neural Network Interpretability Methods}
			Numerous aspects of a neural network can be studied in order to understand the model better. The focus of this thesis is feature importance, which investigates which input variables (features) are important for the prediction. This question is the natural starting point to neural network interpretability, which is why this thesis chooses it as the focal point. This subsection, however, discusses the alternative approaches to neural network interpretability and their applications in finance. 
			
			First, the values in hidden layers can be studied to uncover the feature interactions and patterns learned by the network. Hidden layer's values is a linear combination of the previous layer, with then applied non-linear function. As such, hidden neurons capture the interactions effects of the neurons in the previous layer. If we start with features in the input layer, the first hidden layer captures interactions between all input features. From this perspective, the hidden neurons can be considered as features learned, or engineered, by the network. The hidden layers can be inspected ex post to answer the question of which \textit{interactions} between input features are important. For example, in the case of image recognition, it is possible to see that the network learns to recognize simple shapes (such as straight lines and circles) in the first hidden layers, and then proceeds to learn more intricate patterns (such as flowers and shapes of animals) in the deeper layers \citep{olah2017feature}. In the returns prediction task, it can be analogously studied which feature combinations are important. However, there is no such paper to the best of my knowledge. 
			
			Interestingly, Recurrent Neural Networks (RNNs), can be analyzed in similar manner to uncover the abstract patterns learned by the network. RNNs are used for modeling data with important time dimension, such as voice, text and financial time-series. \cite{di2016artificial} employ RNNs to predict stock price, but without focusing on interpretability. RNN interpretability is demonstrated in \cite{karpathy2015visualizing} on a language model: the authors are able to identify hidden cells that have interpretable meaning, such as neurons that "specialize" in recognizing passages in quotes or brackets. In the financial domain, \cite{giles1997rule} find that RNNs learn well-known patterns in exchange rate movements, such as momentum and reversal. While this thesis views stock returns prediction from the cross-sectional perspective, so these methods are not applicable here, this literature offers promise.  
			
			Another approach to neural network interpretation is constructing a \textit{surrogate model}, an intrinsically interpretable model, such as linear regression, which explains the model's \textit{predictions}, as opposed to explaining the target variable itself (here, such surrogate model would explain the model's returns predictions, as opposed to returns as such.) [TODO expand] 
		 

	\section{Unused text}
		
			A possible definition of ML is from  \cite[p.~2]{mitchell1997machine}: "A computer  program is  said to learn from experience E with respect to  some class of tasks T and performance  measure P, if  its performance at  tasks in T, as measured by P, improves with experience E." In the case of predicting stock returns, the task T is a regression, performance measure can be for example the mean-squared-error, and the experience E is the historical data. Using this definition, ML is a broad collection of methods, including the very simplest, like linear regression or principal components analysis, that are well-known to any economist.  
	   		
	   		Of course, studying only the first-order impact of a characteristic provides only a very limited picture. For example, \cite{bryzgalova2019forest} find that the first-order impact of accruals on returns is flat (zero), but when controlling for size, the relationship turns out to be U-shaped. 
	   		
	   		
	   		
	   		In case of linear model
	   		
	   		\begin{equation}
	   		\hat{f(x)} = \beta_1x_1 + \beta_2x_2 \ldots + \beta_K x_K
	   		\end{equation}
	   		
	   		where $\vec{x}$ are the values for particular observation, the Shapley value of feature $j$ is exactly
	   		
	   		\begin{equation}
	   		\psi_j(\hat{f}) = \beta_jx_j - \beta_jE(X_j)
	   		\end{equation}
	   		
	   		 In case of linear models, the order in which the features "vote" for the prediction is irrelevant (addition is commutative). This is not so in nonlinear models, and this is why the Shapley value takes account for all possible orders in which features can participate in the prediction: in a model with $K$ features, there are $2^K$ possible coalitions between them.
	   		 
	   		 This is done using a global approach to pruning a tree. Global pruning is necessary as the task is to use the tree's nodes to span the stochastic discount factor, which cannot be done using local decision criteria.
	   		 The pruning method proceeds in the following steps. 	
	   		 
	   		 The intermediate and final nodes of the tree then serve as basis assets to construct the discount factors. This is done by optimally weighting the basis assets. The weights are found by finding the minimum variance weights for each expected return, solving the Markowitz problem with additional shrinkage of the weights using elastic net penalty, choosing the the optimum expected return and shrinkage parameters to maximize the Sharpe ratio on validation data. Specifically, the approach takes three steps: 
	   		 
	   		 First, for each node of the tree (including the first and intermediate nodes, i.e., not just the leaves), calculate the sample estimates of mean and covariance matrix of the excess returns, denote them $\hat{\vec{\mu}}$ and $\hat{\vec{\Sigma}}$ respectively.
	   		 
	   		 Second, find weights $\vec{w}$ of the nodes such that the resulting portfolio will have minimum variance, at least a given mean return, and such that the weights are small (elastic net shrinkage). That is, for given $\mu_0$, and shrinkage parameters $\lambda_1$, $\lambda_2$, find weights $\vec{w}$ that solve 
	   		 
	   		 \begin{equation}
	   		 \begin{aligned}
	   		 \min_{\vec{w}} \quad 
	   		 & \frac{1}{2}\vec{w}^{T} \hat{\vec{\Sigma}} \vec{w} + \lambda_1 \norm{\vec{w}}_1 +  			\frac{1}{2} \lambda_2 \norm{\vec{w}}_2 \\
	   		 \textrm{subject to} \quad 
	   		 & \vec{w}^{T} \mathds{1} = \mathds{1} \\
	   		 & \vec{w}^{T}\hat{\vec{\mu}} \geq \mu_0   \\
	   		 \end{aligned}
	   		 \end{equation}
	   		 
	   		 Third, the parameters $\mu_0$, and shrinkage parameters $\lambda_1$, $\lambda_2$ are selected on validation set such that the Sharpe ratio is maximized. 
	   		 
	   		 \cite{gu2020empirical} apply a wide range of ML methods to the stock-returns prediction task. Their model is 
	   		 
	   		 \begin{equation}
	   		 r_{i, t+1} = E_t(r_{i,t+1}) + \epsilon_{i,t+1}
	   		 \end{equation}
	   		 
	   		 where 
	   		 
	   		 \begin{equation}
	   		 E_t(r_{i,t+1}) = g(\vec{z}_{i,t}) = g(\vec{x}_t \otimes \vec{c}_{i,t})
	   		 \end{equation}
	   		 
	   		 
	   		 The function $g$ stands for any ML model and the variables $\vec{z}_{i,t}$ are 94 characteristics of stocks $ \vec{c}_{i,t}$ interacted with 8 macroeconomic predictors $\vec{x}_t$. The multiple-factor model \ref{ER_betalambda} is a special case of this specification, with $g$ linear: 
	   		 
	   		 \begin{equation}
	   		 E_t(r_{i,t+1}) = g(\vec{z}_{i,t}) = \vec{c}'_{i,t}\Theta_1'\Theta_2 \vec{x}_t = \vec{{\beta}}'_{i,t} \vec{\lambda}_{t}
	   		 \end{equation}
	
			The task is to predict the return of given stock in month $t+1$ given that stock's anomalies as of the time $t$. (Two technical notes: first, the dataset is organized so that this shift is taken care of, i.e., the features corresponding to the given target have the same index. Second, the anomalies as of the time $t-1$ can be (and often are) calculated from raw data based on several preceding time periods, e.g., $t$, $t-1$, $t-2$ and therefore the time index  represents the entire information set rather than financial or accounting information being \textit{published} at that time index. For example, a single observation at a given time index $t$ consists of the target (stock's return in $t+1$), and anomalies calculated as of $t$, such as average return in the last six months ($t$, $t-1$, $t-2$, $t-3$, $t-4$, $t-5$))


