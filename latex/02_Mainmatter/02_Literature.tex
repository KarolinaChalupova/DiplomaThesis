\chapter{Literature Review}
\label{chap:lit} 
		
 	The question of what drives the stock returns is a thrilling one to answer from several perspectives: that of an economist, of a statistician, of a machine learning engineer, and that of a finance practitioner.
 	An economist sees that stock returns reflect human decisions: putting a price tag on uncertain future payoffs reflects the trade-off between current and future consumption, human impatience and attitudes to risk \citep{cochrane2009asset} as well as human behavioral biases [TODO add reference]. She also sees that they speak of the complex web of relations between firms: as companies form relationships, they become exposed to similar risks, and their returns correlate. For the economist, studying the determinants of stock returns therefore is an opportunity to understand human behavior as well as the macro-phenomena that emerge in the network of firms' relationships. 
 	The statistician sees that the determinants of stock returns are notoriously over-studied, which brings many issues of publication bias and multiple hypothesis testing, leading to many false discoveries \citep{harvey2016and}. From this perspective, the question stands: how to separate the wheat from the chaff? 
 	The machine learning engineer sees the problem as a prediction task: there is a large amount of data, the variables likely interact in complex ways, and are highly correlated. Machines have proven to excel in the task of predicting stock returns. They learn from financial data, model the interactions, and produce unparalleled returns predictions \citep{gu2020empirical}.
 	Finally, for the finance practitioner seeks to identify the future winners and losers, and sell and purchase them to make profit. As the machine-learning approaches are becoming the norm in the applied field, a need arises to understand the models on a deeper level â€“ to interpret them, which brings us back to the economic underpinnings of stock returns.
 	
 	This thesis attempts to bring the four approaches together. I use machine learning models to predict stock returns and then interpret them to bring us closer to the answers to the economists' questions. This approach can also help to filter out false discoveries. Finally, the finance practitioner can employ ML interpretability methods to find an intelligible model, of which she understands the weaknesses and sources of performance. 
 	
 	The literature review proceeds as follows. First, I review the economic theory behind asset prices and common attitudes to modeling them. Second, I present the statistician's view: the multidimensional challenge faced in modeling stock returns \citep{cochrane2011presidential} and show what different approaches have been used to answer the challenge. Third, I present the machine learning approaches to asset pricing. Fourth, I review ML interpretability methods. 
 	
 	\section{Modeling Stock Returns -- Which Predictors Should Matter?}
 	
	 	Valuing a stock amounts to putting a price tag on a stream of future payoffs. The field concerned with doing just that is called asset pricing. Consider the following basic asset-pricing equation \citep{cochrane2009asset}:
	 	
	 	\begin{equation}
	 		p_t^i = E_t(m_{t+1} x_{t+1}^i ) \label{eq:pEmx}
	 	\end{equation}
	 	
	 	In words, the price of an asset $i$ at time $t$ ($p_t^i$) is proportional to the expected asset's payoff at time $t+1$ ($x_{t+1}^i$). However, since the payoff does not come now ($t$), but in future ($t+1$), we discount it to present by the factor $m_{t+1}$, called the \textit{stochastic discount factor}. The term stochastic stands to express the idea that $m_{t+1}$ is not known with certainty at time $t$. There is only one assumption made in order to write the equation: it can be shown that $m_{t+1}$ exists if and only if the \textit{law of one price} holds, that is, if two assets generate the exact same payoff in all possible states of nature have the same price. If this is the case, there is a discount factor such that the equation holds for all assets $i$ \citep{cochrane2009asset}.
	 	
	 	In the stock market, equation \ref{eq:pEmx} translates to:
	 	
	 	\begin{equation}
	 		1 = E_t(m_{t+1} R_{t+1}^i ) \label{eq:1EmR}
	 	\end{equation} 
	 	
	 	In words, an investor pays 1 dollar now to collect $R_{t+1}^i$ dollars in the future. The payoff is called gross return and is the sum of future price and dividend $R_{t+1}^i = p_{t+1}^i+ d_{t+1}^i$. 
	 	Using the definition of covariance, we can rearrange \ref{eq:1EmR} \citep{cochrane2009asset}:
	 	
	 	\begin{equation}
	 		1 = E_t(m_{t+1}) E_t(R_{t+1}^i) + Cov_t(m_{t+1},R_{t+1}^i) 
	 	\end{equation} 
 	
 		Defining the \textit{risk-free gross return} as $R_{t+1}^f=\frac{1}{E_t(m_{t+1})}$ and using it to further rearrange \citep{cochrane2009asset}:
 		
 		\begin{equation}
 			E_t(R_{t+1}^i) = \underbrace{R_{t+1}^f}_\text{risk-free return}  -  \underbrace{R_{t+1}^f Cov_t(m_{t+1},R_{t+1}^i)}_\text{risk adjustment} \label{eq:risk_adjustment}
 		\end{equation} 
 		
 		This important result shows that expected return if stock $i$ can be decomposed into risk-free return and risk adjustment. Note that the risk adjustment occurs if and only if returns are correlated to the discount factor, so \textit{idiosyncratic} risk, that is, uncorrelated with the discount factor, is uncompensated. Returns positively correlated to the discount factor should be low, and vice versa. So to explain average returns, we "only" need to explain the returns' correlation to the discount factor $m_{t+1}$. 
 		
 		Further rearranging \ref{eq:risk_adjustment} provides one more insight \citep{cochrane2009asset}. Multiply both sides by $Var_t(m_{t+1})/Var_t(m_{t+1})$ to obtain:
 		
 		\begin{align}
	 		E_t(R_{t+1}^i) 
	 		& = {R_{t+1}^f} + 
	 		\underbrace{
	 			\frac{Cov_t(m_{t+1},R_{t+1}^i)}{Var_t(m_{t+1})}
	 			}_\text{denote $\beta_{i,t}$} 
 			\cdot 
 			\underbrace{
 				\left(-\frac{Var_t(m_{t+1})}{E_t(m_{t+1})}\right)
 				}_\text{denote $\lambda_{t}$} \label{eq:factor_model} \\
 			& = \underbrace{{R_{t+1}^f}
 				}_\text{risk-free return} 
 				+ 
 				\underbrace{\beta_{i,t} \cdot \lambda_{t}
 				}_\text{risk adjustment} \label{eq:beta_reprezentation}
 		\end{align} 
 		
 		This equation is so-called \textit{beta-representation} of \ref{eq:1EmR}. It shows that the risk-adjustment, the premium a stock pays for being correlated with the discount factor, can be decomposed into $\beta_{i,t}$ and $\lambda_t$. $\lambda_t$ is the volatility of the discount factor and it is unrelated to properties of asset $i$. It can be interpreted as the price of risk. $\beta_{i,t}$ can be interpreted as the amount of the risk inherent in asset $i$. Empirically, it can be obtained as the coefficient from regressing asset $i$'s returns on the discount factor: 
 		
 		\begin{equation}
 		R_{t+1}^i = a_i + \beta_{i}m_{t+1} + \epsilon_{i,t+1}
 		\end{equation} 
 		
 		Equations \ref{eq:risk_adjustment} and \ref{eq:beta_reprezentation} show that the discount factor is the key to explaining stock returns, as the latter is nothing but compensation for correlation with the former. The following special cases (consumption-based models, Capital Asset Pricing Model, multi-factor and characteristics-based models) offer further insights about the drivers of the stock returns.    
 			 	
	 	\subsection{Consumption-Based Model}
	 	
		 	The consumption-based model takes the additional assumption that the investor's preferences can be captured by her utility from consumption. The investor needs to decide how much to consume today and how much to save for tomorrow. First order conditions for this problem lead to the following specification of the discount factor \cite{cochrane2009asset}: 
		 	
		 	\begin{equation}
		 		m_{t+1} = \kappa \frac{u'(c_{t+1})}{u'(c_t)} \label{consumtion_based_model}
		 	\end{equation}
		 	
		 	where $u'(c_{t+1})$ denotes marginal utility from consumption. That is, the discount factor is the consumer's  marginal rate of substitution between consumption at time $t$ and $t+1$ and captures the willingness to trade consumption today for consumption tomorrow, where parameter $\kappa$ is the weight placed on future utility.  This means that such an investor would demand a high return for stock that perform badly at times when she is unwilling to give up today's consumption, and a low return for stock that performs badly at times she is willing to give up today's consumption. 
		 	
		 	There are many variables that can affect the rate at which the consumer is willing to swap current for future consumption: for instance, the current state of the economy, such as GDP growth, interest rate and current asset returns, as well as news of the \textit{future} states of the economy. 
		 	
		 	For simplification, one can assume constant relative risk aversion:
		 	
		 	\begin{equation}
		 		u(c_t) = \frac{c^{1-\gamma}-1}{1-\gamma}
		 	\end{equation}
		 	
		 	where $\gamma$ is a parameter positive for risk-averse individuals, and plug in to \ref{consumtion_based_model}: 
		 	
		 	\begin{equation}
		 	m_{t+1} = \kappa \left(\frac{c_{t+1}}{c_t}\right)^{-\gamma} 
		 	\end{equation}
		 	
		 	That is, the discount factor is inversely proportionate to consumption growth. \ref{eq:risk_adjustment} says that assets with positive correlation with the discount factor should have lower returns, so it follows that assets highly correlated with consumption growth should earn high returns. This makes sense: the investor is assumed to care only about her marginal utility from consumption. Therefore, she demands a higher return for holding a stock that performs badly at times when her consumption decreases, and only a low return on insurance-like stocks that perform well during bad times.
		 	 
		 	However, this model does not perform very well empirically \cite{cochrane1996cross}. There are two possible reasons, either, the aggregate consumption data are imperfectly measured, or the assumption that the investor's decision making can be described by maximization of utility from consumption is off. The two criticisms are addressed respectively by Capital Asset Pricing Model (or multifactor models) and  
	 	
	 	\subsection{Capital Asset Pricing Model}
	 	
	 		The Capital Asset Pricing Model (CAPM) is a special case of the consumption-based model, making the additional assumption that the utility from consumption is logarithmic \citep{rubinstein1976valuation}:
	 		
	 		\begin{equation}
	 			u(c_t) = ln(c_t)
	 		\end{equation}
	 		
	 		Plugging this to \ref{consumtion_based_model}, one obtains 
	 		
	 		\begin{equation}
	 			m_{t+1} = \kappa \frac{c_t}{c_{t+j}}
	 		\end{equation}
	 		
	 		The CAPM operates with the concept of wealth portfolio, which comprises all world's wealth, including real estate, metals, machinery or art. The price of this portfolio is (by \ref{eq:pEmx} and plugging in for the discount factor):  
	 		
	 		\begin{equation}
	 		p_t^W  = E_t \sum_{j=1}^{\infty} m_{t+j}c_{t+1} = \frac{\kappa}{1-\kappa}c_t
	 		\end{equation}
	 		
	 		and the return on wealth portfolio is therefore 
	 		
	 		\begin{equation}
	 		R_{t+1}^W  = \frac{p_{t+1}^W + c_{t+1}}{p_t^W} = \frac{1}{\kappa} \frac{c_{t+1}}{c_t} = \frac{1}{m_{t+1}}
	 		\end{equation}
	 		
	 		Thus, according to CAPM, the discount factor is the inverse of the return on the wealth portfolio. As \ref{eq:risk_adjustment} says that assets with positive correlation with the discount factor should earn low returns, if follows that assets positively correlated with the wealth portfolio should earn higher returns. Again, this makes sense: an investor who only cares about consumption demands a higher return for stocks that perform badly when other sources of wealth perform badly, and vice versa, she is willing to forgo some average return in exchange for good performance at times when all else fails. Since the return on wealth portfolio is unobservable, it is empirically often replaced by return on market portfolio of stocks. This may be problematic, as wealth comprises many more assets than just stocks, so the approximation is likely too crude \citep{roll1977critique}.  
	 		
	 		
	 	\subsection{Multi-factor Models}
	 		
	 		Multi-factor models are motivated by the notions that consumption growth and return on wealth portfolio do not proxy the discount factor empirically very well (in the consumption-based model and CAPM respectively), and they try to find other proxies for the discount factor $m_{t+1}$, factors $\vec{f}_{t+1}$. 
	 		
	 		\begin{equation*}
	 		m_{t+1} = \approx a+\vec{b}'\vec{f}_{t+1}
	 		\end{equation*}
	 		
	 		
	 		If we retain the idea that investor maximizes her utility from consumption, then the factors  should proxy the rate at which investors are willing to swap current for future consumption \citep{cochrane2009asset}:  
	 		
	 		\begin{equation*}
	 			m_{t+1} = \kappa \frac{u'(c_{t+1})}{u'(c_t)} \approx a+\vec{b}'\vec{f}_{t+1}
	 		\end{equation*}
	 		
	 		Multi-factor models explain returns as	 
	 	
			 	\begin{align}
			 		& E_t(R^i_{t+1}) = R^f_{t+1} + \vec{\beta}'_{i,t} \vec{\lambda}_{t} \\
			 		& R^i_{t+1} = \alpha_{i,t} + \vec{\beta}'_{i,t} \vec{f}_{t+1} + \epsilon_{i,t+1} \label{multifactor model}
			 	\end{align}
			 
			 The \textit{factor loadings} $\vec{\beta}'_{i,t}$ of size $1\times K$ are exposures to risk factors $\vec{f}_{t+1}$ of size $K \times 1$, where $K$ is the number of risk factors. They represent the amount of risk inherent in asset $i$ due to its correlation to the corresponding risk factor. The $\vec{\lambda}_{t}$ of size $K \times 1$ are interpretable as the risk prices at time $t$ \citep{kelly2019characteristics}. (Note that the CAPM and consumption-based models are special cases of the multiple-factor models, where $K$, the number of factors, is 1.) 			
			 
			 First multi-factor model is \cite{fama1996multifactor}:
			 
			 \begin{align}
			 	& E(R^i) = R^f + \vec{\beta}'_{i} \vec{\lambda} \\
			 	& R^i_{t+1} = \alpha_{i} + \vec{\beta}'_{i} \vec{f}_{t+1} + \epsilon_{i,t+1}
			 \end{align}
		 
		 	with factors  
			  		
				\begin{equation}
					\vec{f}_{t+1} = 
						\begin{pmatrix}
							R^M_{t+1}-R^f_{t+1} \\
							SMB_{t+1} \\
							HML_{t+1}				
						\end{pmatrix}
				\end{equation}
			
			and with factor prices 
			
			\begin{equation}
			  \vec{\lambda} =
				\begin{pmatrix}
					E(R^M)-R^f \\
					E(SMB) \\
					E(HML)				
				\end{pmatrix} 
			\end{equation}
			 
			The $R^M_{t+1}-R^f_{t+1}$ is the return on market portfolio above risk-free rate (as in CAPM), $SMB_{t+1}$ is the return on small-cap portfolio minus the return on big-cap portfolio, and $HML_{t+1}$ is the return on portfolio of stocks with small market values relative to book value, minus the return on portfolio of stocks with high market values relative to book value. A possible explanation why $SMB$ and $HML$ should be correlated to the discount factor is that they forecast the future state of the economy \citep{liew2000can}.		
			\cite{fama2015five} augment this three-factor model to five factors. 
			
			
		\subsection{Modeling Stock Returns with Characteristics}
			
			It is difficult to empirically estimate a multifactor model. The main challenge is that both $\vec{f}_{t+1}$ and $\vec{\beta}'_{i,t}$ are unobserved, so we have no directly observed variables at the right hand side of the estimated equation. Some researchers tackle the issue by using prior empirical knowledge to pre-specify the factors  \citep{fama1993common, fama2015five}, others use instrumental variable approach \citep{kelly2019characteristics} to uncover the unobserved factor loadings using firm-level variables, commonly called \textit{characteristics}, which serve as proxies for exposure to systematic risks \cite{kelly2019characteristics}:  
			
			\begin{align}
				E_t(R^i_{t+1}) = R^f_{t+1} + \vec{\beta}'_{i,t} \vec{\lambda}_{t}  \label{eq:ER_betalambda} \\
				R^i_{t+1} = \alpha_{i,t} + \vec{\beta}'_{i,t} \vec{f}_{t+1} + \epsilon_{i,t+1} \\
				\vec{\beta}'_{i,t} = g(\vec{z}'_{i,t}) \label{eq:instrumented_betas}
			\end{align}
			 
						
			An important distinction between characteristics and factors is that characteristics are firm-specific variables, such as firm size, returns momentum or book-to-market ratio, while factors are common to all firms, such as the market return. Additionally, while characteristics proxy a firm's \textit{exposure} to risk (equation \ref{eq:instrumented_betas}), factors are the risk sources themselves. 
			
			Often, characteristics alone are used to model returns (e.g., \cite{gu2020empirical, bryzgalova2019forest, tobek2020does}). This is typically done when the focus of the research is returns prediction rather than the factor structure:
			
			\begin{equation}
				E_t(R^i_{t+1}) = f(\vec{z}'_{i,t})
			\end{equation}
			
			
			There are hundreds of published factors and characteristics suggested as explanation of average stock returns. \cite{harvey2016and} count 313 variables, considering the top journals only and calling the count "surely too low". Indeed, \cite{cochrane2011presidential} refers to the state of asset pricing as a "zoo of factors". At the same time, the number of factors should be low in theory \citep{cochrane2011presidential} and empirically \citep{ahn2012determining}. Actually, many of these published return predictors are spurious. As \cite[p.~5]{harvey2016and} put it rather famously: "most claimed research findings in financial economics are likely false". Main reasons for this include publication bias and data-snooping bias (multiple-hypothesis testing bias, in-sample overfitting) and lack of replication studies in finance \citep{harvey2016and, mclean2016does}. 12\% of variables cannot be replicated even in-sample, the rest is biased by about 10\% \citep{mclean2016does}. \cite{harvey2016and} show that out of 296 published significant factors 158, 142, 132 and 80 are false discoveries, the precise number depending on statistical framework used.
			
			\citeauthor{cochrane2011presidential} formulates the problem of separating the wheat from the chaff in stock returns prediction in his "multidimensional challenge" \citep[p.~1060]{cochrane2011presidential}: "We have a lot of questions to answer: First, which characteristics really provide independent information about average returns?" In response to this challenge, studies emerged that study all the published characteristics in a single model, which allows to their effects to crows each other out in a 'survival of the fittest', such as \cite{gu2020empirical} and \cite{tobek2020does}. 
		
			\cite{gu2020empirical} and \cite{tobek2020does} study 94 (153) characteristics proposed by prior literature and use them in various return-prediction models (linear regression, gradient-boosting regression trees, random forests and neural networks). Their results suggest that some categories of characteristics are consistently more important than others across time and model specifications. This finding presents a crucial stepping stone for this thesis, for two reasons. First it narrows down the search for return predictors to a set that is more feasible to work with: this thesis selects 30 most important variables from \cite{tobek2020does} as stock return predictors. \footnote{There are three advantages of \cite{tobek2020does} over \cite{gu2020empirical} for this purpose: First, like this thesis,  \cite{tobek2020does} use a liquid universe of stocks, which means that their results are less likely to be driven by transactions costs. Second, their data are global, rather than US only, which allows to uncover more generally applicable findings. Finally, they use larger set of characteristics (153 rather than 93), which makes it less likely that important information is omitted.} Second, the different models agree on important variables, which allows this thesis to focus on neural networks without much loss of generality. 
		
		\subsection {Economic Motivation of Predictors Used in This Thesis}
			\label{chap:economic_motivation_of_predictors}
			The predictors used in this thesis fall into several categories by their economic motivation. These categories come up very consistently across literature as important for predicting stock returns \cite{gu2020empirical, tobek2020does, harvey2016and, mclean2016does} and can be considered the current answer to Cochrane's multidimensional challenge of which characteristics provide information about stock returns. This section presents these categories and their economic rationale and at the same time introduces the particular characteristics used in this thesis as members of these broader categories. (The term characteristics, predictors, and features is used interchangeably throughout the thesis.)
			
			The content of the section is summarized in Table \ref{tab:characteristics_motivation}. It shows all the predictors used in this thesis (column Features), together with their authors and publishing journal, categorized by their economic motivation (column Category). Each category is discussed in detail its own paragraph below. 
			
			\begin{table}
				\resizebox{\textwidth}{!}{\input{Tables/characteristics_motivation.tex}}
				\caption{Economic Motivation of Predictors Used in This Thesis}
				\label{tab:characteristics_motivation}
				\medskip
				\small 
				The table shows all the characteristics (features) used in this thesis grouped by their underlying economic motivation as predictors of stock returns. Column Category gives the main economic motivation of the variable and is discussed in detail in the text of this section (\ref{chap:economic_motivation_of_predictors}). 
			\end{table}
			
			A first group of characteristics relates to how investors approach volatility. Traditional asset pricing theory assumes investor's approach to risk is symmetrical in gains and losses \citep{cochrane2009asset}. However, psychology suggests that people are at the same time risk-averse (case of insurance) and risk-loving (case of lottery) and have asymmetrical utility in gains and in losses \citep{kahneman2013prospect}. This is in contrast to the standard utility maximization assumption of the Consumption-Based model and Capital Asset Pricing Model discussed above. This has direct implications for stock returns: it seems that lottery-like stocks, stocks with right skew and stocks with low correlation with market volatility can afford to pay lower average returns in exchange for their appealing risk profile (predictors \textit{Maximum Return} \citep{bali2011maxing}, \textit{Coskewness} \citep{harvey2000conditional} and \textit{Idiosyncratic Risk} \citep{ang2006cross}, respectively).
			
			A second group of characteristics also relates to the behavioral science. It seems that investors are slow to revise their existing beliefs upon arrival of new information, which shows in stock returns as momentum (predictors \textit{52-Week High} \citep{george200452} and \textit{Lagged Momentum} \citep{novy2012momentum}), and when they \textit{do} revise their beliefs, they over-react, which shows as reversal -- periods of high returns followed by periods of low returns (predictors \textit{Short-Term Reversal} \citep{jegadeesh1990evidence} and \textit{Momentum-Reversal} \citep{jegadeesh1993returns}). 
			
			A third and fourth group of characteristics relate to liquidity. It appears that illiquid stocks (typically small stocks with low trading volume and large bid-ask spread) must offer a higher return to compensate for their higher trading costs (predictors \textit{Amihud's Measure} \citep{amihud2002illiquidity}, \textit{Liquidity Shocks} \citep{bali2013liquidity} and \textit{Volume over Market Value of Equity}  \citep{haugen1996commonality}). It seems that investors avoid illiquidity to the degree a that mere higher chance of a stock becoming illiquid commands a higher return (predictors \textit{Coefficient of Variation of Share Turnover} \citep{chordia2001trading} and \textit{Liquidity Beta 3} and \textit{Liquidity Beta 5}  \citep{acharya2005asset}).  
			
			A fifth group of characteristics focuses is due to limited attention of investors, which can result in mis-pricing. First, it seems that investors tend to overlook artificially bloated (manipulated or "dressed") balance sheets, and tend to over-price firms with high accruals (predictors \textit{Accruals} \citep{sloan1996create}, \textit{Net Operating Assets} \citep{hirshleifer2004investors}, \textit{Operating Profits to Assets} \citep{ball2016accruals}, and \textit{Change in Common Equity} \cite{richardson2006implications}). Similarly, they tend to under-value firms with high research and development costs (which are not present on the balance sheets) (predictor \textit{ RD Over Market Equity} \cite{chan2001stock}). Finally, limited attention of investors manifests in overlooking important decompositions of profit margin (predictor \textit{Profit Margin} \cite{soliman2008use}).
			
			A sixth group of characteristics reflects the empirically observed seasonal patterns in stock returns. While the theoretical underpinning of these characteristics is still a matter of academic debate, the phenomenon is very strong and robust empirically (\textit{Seasonality} predictors from \cite{heston2008seasonality}). 
 			
 			A seventh group of characteristics are proxies for the well-known value effect \cite{fama1993common}, which shows that empirically, firms with high Book to Market ratio tend to have higher returns. The predictors used here are \textit{ Duration of Equity }\citep{dechow2004implied}) and \textit{Leverage Component of Book to Price} \citep{penman2007book}, and their original papers discuss also the theoretical underpinnings of the phenomenon.
 			
 			Finally, financially constrained firms tend to co-move, and since financial constraints are typically market-wide during economic crises, this risk is not diversified, and therefore priced. The predictor \textit{Whited-Wu Index} \citep{whited2006financial} measures the extent to which a stock is impacted by financial constraints.  
			
				
		\section{Why use ML for Stock Returns Prediction?}
			
			One of the reasons behind popularity of ML in stock returns prediction is that it offers superior performance. \cite{gu2020empirical} make a horse race of several ML and traditional methods. First, they include methods to estimate linear $g$, both without regularization (linear regression with all variables (OLS), linear regression only with size, book-to market, and mometum (OLS-3)) and with regularization (partial least squares (PLS), principal components regression (PCR), elastic net (ENet)). Second, they include methods to estimate nonlinear $g$: generalized linear models (GLM), random forest, gradient-boosted regression trees, and neural networks of depths 1 to 5. They find that OLS performs badly, regularized linear models are an improvement, and, and nonlinear models are the best-performing, namely neural networks followed by random trees. 
			
			Second, there is strong theoretical appeal to using ML as well. Additionally, prior research shows that linear models miss important information about stock returns by ignoring the interactions of the predictors. If a variable has a non-linear relationship with returns, the linear models can conclude there is no association, while in reality there is an important non-linear relationship. For example, in \cite{gu2020empirical}, linear models deem size and volatility unimportant predictors, while non-linear models find the contrary. A similar problem may arise if predictors are have zero association with return themselves, but not in interaction with other predictors. The nonlinearity seems to be a curcial aspect of stock return prediction: \cite{bryzgalova2019forest} and \cite{gu2020empirical} agree that the ability to uncover non-linearities and interactions is the crucial driving force of the superior ML performance and \cite{gu2020empirical} statistically reject linear models in favor of the non-linear ones as those with better performance. As there are so many possible forms in which the predictors can interact, it is unfeasible to search for the correct specification manually. ML offers methods of choosing the right functional form systematically. 
			
			Moreover, the model selection is ML is based on validation data, as opposed to in-sample (training) data. In traditional econometrics approach to stock returns, model is tuned by researcher based on in-sample fit, which results in spurious findings that are very often not replicable out-of-sample \citep{mclean2016does}. On the other hand, ML puts an emphasis on out-of-sample performance, which allows to select a robust model. 	
			
			Finally, ML methods are designed to handle data with high dimensionality and correlation in predictors. ML offers dimension reduction and variable selection methods to overcome this challenge, and thus presents a natural way of approaching Cochrane's multidimensional challenge \citep{cochrane2011presidential}. 	
			
	
	\section{Interpretable Machine Learning Methods}
	
		To explain how the methods employed in this thesis fit into the broader context of ML interpretability literature, it is useful to first categorize them following \cite{molnar2020interpretable}. A first dictinction can be made between \textit{intrinsically} interpretable models, which are intrepretable per se (linear and logistic regression, random trees), and \textit{post-hoc methods}, that can be applied to extract interpretable knowledge from more complex models, such as neural networks, which is the case of this thesis. A further distinction can be made as to the applicability of the interpretability methods: some methods are \textit{model-specific}, that is, can only be applied to some ML models, while others are \textit{model-agnostic}, applicable to all models from linear regressions to random trees and neural networks. For purposes of generality, the methods selected in this thesis are model-agnostic --- while they are well-suited to neural networks, they can also be used on any other model, such as linear regression. Yet other distinction can be made between \textit{global} and \textit{local} ML interpretability methods: while the former study how a model decides overall (across all observations), the latter can be used to interpret the drivers behind a single observation. Both approaches are taken in this thesis, as both global and local perspectives are important for model interpretability.
			
			
		\subsection{Interpretable Machine Learning Methods and Stock Returns: What Has Been Done?}
			\cite{bryzgalova2019forest} use an intrinsically interpretable model to study stock returns. Rather than predicting stock returns as do \cite{gu2020empirical} and this thesis, the ML model is used here to build the discount factor (see the section on Asset Pricing Theory for what is the discount factor). The model used is random tree. A random tree iteratively splits data into two parts, using a particular feature and its particular value to make each split. It starts with the entire dataset, performs the first split in two parts, the two parts are then split in two again etc. recursively, until a stopping criterion is met. The latest, unsplit groups of the dataset, called leaves, are then groups of the data that share the same prediction. \cite{bryzgalova2019forest} note that this is a generalization of the portfolio sorts, which is a method typically used in finance to arrive at the discount factor. The leaves of the tree are used as basis assets for forming the discount factor. Since the splits are not made by the researcher, but instead learned by the model itself, this presents a viable approach to forming discount factor from multi-dimensional data. This overcomes the issue with portfolio sorts, which break down when the dimensionality of the data, i.e., number of features, is large \citep{cochrane2011presidential, bryzgalova2019forest}. The advantage of random trees is that they are quite easily interpretable: to understand how prediction is made for a single feature, it suffices to follow the path the feature takes through the tree from the root to the leaves, always classified into group A or B based on a single feature. 
			
			Random trees are also used for returns prediction. In this case, the resulting leaves are not used to form basis assets as in \cite{bryzgalova2019forest}, but the firms inside a leaf share the return prediction. [TODO find a paper, there must be something.] \cite{bryzgalova2019forest} study recent price trends (momentum and long- and short-term reversal), liquidity measures (such as turnover), investment, profitability, accruals and book-to-market ratio. [TODO find important, compare to Gu]. 
			
			\cite{gu2020empirical} 
			
			
			
				
		\subsection{Choosing Local Feature Importance Measure}
			Local feature importance measures aim to answer the question of why the model gives a particular prediction for a particular observation. For example, why does the model predict Apple's return in the following month to be 15\%? In particular, how do the individual features contribute in magnitude and sign to the prediction? In other words, how can the prediction be decomposed into (\textit{attributed to}) individual features? For this reason, the task is also called \textit{feature attribution}.  
			
			There are several properties that are naturally desirable for any attribution measure to have. First, we would like the measure to give a complete account of the prediction: if we sum the individual feature attributions, we would like to obtain the final prediction (called Completeness or Efficiency axiom in \cite{sundararajan2017axiomatic} and \cite{molnar2020interpretable} respectively). Second, the measure should give zero attribution score to any feature that is mathematically independent of the predicted variable (called Sensitivity(b) or Dummy axiom in \cite{sundararajan2017axiomatic} and \cite{molnar2020interpretable} respectively). A related desirable property is that if two observations differ in one feature and have different prediction, the attribution to the differing feature should be non-zero (called Sensitivity(a) axiom in \cite{sundararajan2017axiomatic}). Third, the measure should be additive: if a model is a linear combination of other models (such as an ensemble model), the attributions of the model should be the same linear combination of the attributions of the individual models (called Linearity or Additivity axiom in \cite{sundararajan2017axiomatic} and \cite{molnar2020interpretable} respectively). Fourth, if two models with different architectures produce the same predictions for the same set of inputs, the attributions of the two models should be identical (called Implementation Invariance axiom in \cite{sundararajan2017axiomatic}). As pointed out by \cite{sundararajan2017axiomatic}, it is particularly important in the case of neural networks that Implementation Invariance be upheld: a single functional form of the model can be attained using more than one set of weights due to many degrees of freedom. The training of the network can converge to any of these sets, depending, for instance, on random initialization of weights. However, if the function arrived at is effectively the same, it is desirable that the feature attribution be the same as well. Finally, symmetric features (i.e., such that $f(x,y) = f(y,x) \forall x, y$) should receive the same attributions if $x=y$ for a particular observation (called Symmetry axiom in \cite{shrikumar2017learning} and  \cite{molnar2020interpretable}. 
			
			Several local feature importance measures are put forward in the literature. \textbf{Simple Gradient Method} \citep{baehrens2010explain} applies a first order linear approximation of the model to find the sensitivity of the prediction to small changes in values of a given feature. It is a natural starting point for computing feature attribution, as it is very intuitive: if a small change in predictor's value has large effect on prediction, the gradient is large. Other approaches involve back-propagating the final prediction through all the layers down to the input layer. These include  \textbf{Guided Back-Propagation} \citep{springenberg2014striving}, \textbf{Layer-Wise Relevance Propagation} \citep{binder2016layer} and \textbf{DeepLift} \citep{shrikumar2017learning}. However, all these methods violate the most fundamental axioms, rendering the usage of the methods very problematic. Simple Gradient Method and Guided back-propagation violates Sensitivity(a) and Completeness and DeepLift and Layer-Wise Relevance violate Implementation Invariance \citep{shrikumar2017learning, sundararajan2017axiomatic}.
			
			There are only two methods that satisfy the above axioms: Integrated Gradients \citep{sundararajan2017axiomatic} and Shapley (Shapley-Shubik) Values \citep{shapley1971assignment}. 	
			
			\textbf{ Integrated Gradients} \citep{sundararajan2017axiomatic} is a method similar to Simple Gradient Method in that it considers the gradient of the prediction with respect to the feature values. The gradient captures how the prediction changes for small changes in given feature. (Using the gradient is advantageous as it must be already calculated (with respect to weights) during network training.) The measure alters the Simple Gradient Method by considering the gradient not only exactly at the observation, but also at all points on the path from a baseline (usually $0$) to the observation at hand. This adjustment is already enough to achieve Completeness and Sensitivity(a) axioms that Simple Gradient Method breaks. 
			
			\textbf{Shapley Values} originate in game theory and their use in ML is intuitively explained in \cite{molnar2020interpretable}: for a single observation, imagine that the feature values enter a room in random order. At each point, all feature values present in the room make a prediction. Shapley value for feature $j$ is the average change in prediction that happens when feature $j$ enters the room. The average here can be taken either across all possible random orders as in \cite{shapley1971assignment} (there are $K!$ of them for $K$ features) or approximated using several samples as in \cite{vstrumbelj2014explaining}. 
			
			Actually, it can be shown \citep{sundararajan2017axiomatic} that the axioms of Completeness, Implementation Invariance, Sensitivity and Linearity are satisfied \textit{only} by a particular group of methods, called \textit{path methods}, which are all very much like Integrated Gradients, but may take other than straight-line path between the baseline and the observation at hand. Within path methods, Integrated Gradients can be shown \citep{sundararajan2017axiomatic} to be the only one that is symmetry preserving. Shapley Values are related to the path methods in that they average over \textit{multiple} paths, instead of just considering a single path. This is unfortunately the reason behind their computational intensity, as discussed below.  
			
			Integrated Gradients have one crucial practical advantage over Shapley Values. Even the approximate version of Shapley Value \citep{vstrumbelj2014explaining} is very computationally intensive: sampling a single random order of features then takes $K$ calls to the network --- a prediction is made \textit{each time} a feature "enters the room". So to calculate even the approximate Shapley Value for a single observation, the network must be called $K\cdot M$ times, where $M < K!$ is the number of steps in the approximation. On the other hand, the Integrated Gradients only take $M$ calls to the network. In case of many features and (or) many observations for which we wish to calculate local feature importance, and (or) long prediction times (note that all three are often the case in ML), the difference in computation times is crucial. Even though the number of features is only 30 in this thesis, the difference in computation times is already substantial. Moreover, much more features are to be expected in practice, so using a scalable measure seems vital. Therefore, this thesis uses Integrated Gradients as the local feature importance measure. The technical aspects of calculating Integrated Gradients are described in the Data and Methodology. 
		
		
		\subsection{Global Feature Importance Measure} 
		
			\paragraph{Mean Decreased Performance} proposed by \citep{fisher2019all}, also known as \textbf{Permutation Feature Importance} is a model-agnostic measure of feature importance, which measures how much of model's performance is lost when permuting a given feature. It compares the performance of a model fit on a set of features with the performance of the same model after permuting a given feature (randomly shuffling its features across examples, without re-fitting the model). If the particular feature is important, permuting it will decrease model's performance. There are several special cases depending on the measure the model's performance, e.g., \textbf{Model Reliance} \citep{fisher2019all} (mean squared error), or \textbf{Mean Decreased Accuracy} [TODO add citation] (accuracy). Since the measure depends on random permutation, its variance can be decreased by computing it several times and averaging the result. [TODO add alogorhitmic description.]	
			
			\paragraph{Mean Decreased Explained Variance} is also a model-agnostic measure of feature importance, and measures how much of model's explained variance is lost when replacing the feature's values.  It is different to Mean Increased Loss in that rather than measuring changes in model's \textit{error}, it measures the decrease in explained variance of the target, or $R^2$. It is used for example by \cite{gu2020empirical} or \cite{kelly2019characteristics}. [TODO find original methodological paper introducting the measure].  \cite{gu2020empirical} use replacement of values by 0, as their features are scaled from $-1$ to $1$, but generally a different replacement is needed [TODO could be permutation? I think so. In that case average needs to be taken.]
			
			\paragraph{Sum of Squared Derivatives} proposed by \cite{dimopoulos1995use} is the sum of squared partial derivatives of the model's prediction with respect to the given feature. The disadvantage of this measure that is its not model-agnostic, specifically it is unusable if derivatives cannot be taken such as in random trees. 
			
			\paragraph{Mean Decreased Impurity} proposed by [TODO find out] can be used for tree-based methods, unlike Sum of Squared Derivatives.  
			
		
		\subsection{Other Neural Network Interpretability Methods}
			Numerous aspects of a neural network can be studied in order to understand the model better. The focus of this thesis is feature importance, which investigates which input variables (features) are important for the prediction. This question is the natural starting point to neural network interpretability, which is why this thesis chooses it as the focal point. This subsection, however, discusses the alternative approaches to neural network interpretability and their applications in finance. 
			
			First, the values in hidden layers can be studied to uncover the feature interactions and patterns learned by the network. Hidden layer's values is a linear combination of the previous layer, with then applied non-linear function. As such, hidden neurons capture the interactions effects of the neurons in the previous layer. If we start with features in the input layer, the first hidden layer captures interactions between all input features. From this perspective, the hidden neurons can be considered as features learned, or engineered, by the network. The hidden layers can be inspected ex post to answer the question of which \textit{interactions} between input features are important. For example, in the case of image recognition, it is possible to see that the network learns to recognize simple shapes (such as straight lines and circles) in the first hidden layers, and then proceeds to learn more intricate patterns (such as flowers and shapes of animals) in the deeper layers \citep{olah2017feature}. In the returns prediction task, it can be analogously studied which feature combinations are important. However, there is no such paper to the best of my knowledge. 
			
			Interestingly, Recurrent Neural Networks (RNNs), can be analyzed in similar manner to uncover the abstract patterns learned by the network. RNNs are used for modeling data with important time dimension, such as voice, text and financial time-series. \cite{di2016artificial} employ RNNs to predict stock price, but without focusing on interpretability. RNN interpretability is demonstrated in \cite{karpathy2015visualizing} on a language model: the authors are able to identify hidden cells that have interpretable meaning, such as neurons that "specialize" in recognizing passages in quotes or brackets. In the financial domain, \cite{giles1997rule} find that RNNs learn well-known patterns in exchange rate movements, such as momentum and reversal. While this thesis views stock returns prediction from the cross-sectional perspective, so these methods are not applicable here, this literature offers promise.  
			
			Another approach to neural network interpretation is constructing a \textit{surrogate model}, an intrinsically interpretable model, such as linear regression, which explains the model's \textit{predictions}, as opposed to explaining the target variable itself (here, such surrogate model would explain the model's returns predictions, as opposed to returns as such.) [TODO expand] 
		 

	

