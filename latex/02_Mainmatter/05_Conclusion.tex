\chapter{Conclusion}
\label{con}

The recent explosion in ML interpretability methods offers new opportunities to explain stock returns within the framework of the most powerful approach to modeling them: the neural network. This thesis pioneers the exploration of two of these newly opened opportunities.

First, the thesis extracts new insights about the problem that has been occupying finance for over five decades: which variables explain differences in the cross-section of stock returns? Prior research offers preliminary findings of which variables are important according to neural networks \citep{gu2020empirical, tobek2020does}. This thesis investigates 30 predictors that appear most important in existing research more deeply. In particular, it measures the typical sign of the effects that the predictors have on the return in neural networks. This is important, as it allows to see whether neural networks go \textit{with}, or \textit{against}, the economically motivated theoretical signs of the predictors. If we (safely) ignore the 4 least important variables, the networks agree in the sign in as much as 22 out of 26 cases, which means that they overwhelmingly support the economic mechanisms suggested by the asset-pricing literature. Additionally, the thesis investigates more deeply than prior literature the overall importance of the variables in the network, by choosing an interpretable metric well grounded in ML theory and studying the results in various decompositions. Financial constraints, behavioral biases of investors, risk of illiquidity and value effect appear to be the most important drivers of stock returns uncovered by the networks. At the same time, there are several pieces of evidence that \textit{Whited-Wu Index}, \textit{Short-Term Reversal} and \textit{Maximum Return} may have a more complex relationship to returns then previously thought. The results are stable across model architectures, ensemble components as well as time periods and narrow down the search for important determinants of stock returns.

Second, this thesis explores the sources of the unparalleled ability of neural networks to construct profitable long-short portfolios. This offers first insights into these state-of-the-art models that are increasingly used in the financial practice. The thesis shows how any single return prediction can be decomposed into its drivers and also computes a novel metric, Portfolio Reliance, allowing to see which variables the networks consider important to identify future winners and losers among the stocks in the market. A decomposition in time further uncovers that these variables change from year to year, unlike the overall, or cross-sectional, importance of the features. The networks constructed in this thesis offer comparable profitability to state-of-the-art models both in terms of mean return and Sharpe ratio, which makes them very relevant to a finance practitioner. The use of a highly liquid universe of stocks further increases the relevance of the results for trading.   

The areas for further research are numerous. First, running all results again on simulated data would bring a greater insight into the soundness of the interpretability measures: when the researcher has complete knowledge of the data generating process, she can perfectly asses whether the selected measures perform well. Second, it could be investigated what is the exact \textit{form} of the non-linearities: which interactions of variables are important and what are the exact functional forms of the relationship between the predictors and stock return found by the networks. Third, it should be studied whether the interpretation of the networks in asset-pricing is susceptible to adversarial attacks, as is the case of much more complex networks used for image recognition \citep{ghorbani2019interpretation}. Finally, and this is likely the most thrilling area of future research, the statistical properties of feature importance measures in ML are currently quite unclear. More research -- e.g. in the direction of \cite{fisher2019all} -- could bring about an exciting union of statistics and interpretable machine learning.   



