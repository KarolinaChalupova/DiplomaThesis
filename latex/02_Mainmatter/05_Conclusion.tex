\chapter{Conclusion}
\label{con}

Prior literature shows that neural networks can find patterns in current information and use them to predict future stock returns -- and that they do that better than any other model \citep[e.g.][]{gu2020empirical, tobek2020does}. At the same time, neural networks are likely the most complex models in existence. This is both their advantage -- unlike linear models, they are able to uncover non-linear and interaction relationships in the data, which is likely the key to their success  \citep[e.g.][]{bryzgalova2019forest, gu2020empirical} -- and their curse: the complexity makes the model less readily interpretable. This is problematic, as interpretability is key to understanding the strengths and weaknesses of any model and to using it properly. 

However, neural networks \textit{can} be interpreted. There is an explosion of ML interpretability methods proposed in the literature \citep[e.g.][]{molnar2020interpretable}. In the domain of stock returns prediction, this opens two major opportunities. First, it is an opportunity to extract insights about the problem that has been consuming the academia for over five decades: which variables explain differences in the cross-section of stock returns \citep{cochrane2011presidential}? Second, it allows to understand the sources of performance of state-of-the-art models that are increasingly used in the financial practice. This thesis explores both these opportunities, and is likely the first one to do so at this large a scope.   

The contribution for the asset-pricing literature is two-fold. Prior research offers preliminary findings of which variables are important to explain differences in stock returns in neural networks \citep{gu2020empirical, tobek2020does}. This thesis investigates 30 predictors that appear most important in existing research more deeply. First, the thesis is to the best of my knowledge the first to measure the typical sign of the effects that the predictors have on the return in neural networks. This is important, as it allows to see whether neural networks go \textit{with}, or \textit{against}, the economically motivated theoretical signs of the predictors. If we (safely) ignore the 4 least important variables, the networks agree in the sign in as much as 22 out of 26 cases, which means that they go overwhelmingly \textit{with} the economic motivations. Second, the thesis investigates more deeply than prior literature which variables are most important in the models overall, by choosing an interpretable metric well grounded in ML theory. The results are stable across model architectures and time periods and greatly narrow down the search for important determinants of stock returns. Financial constraints, behavioral biases of investors, risk of illiquidity and value effect appear to be the most important drivers of stock returns uncovered by the networks. 

There is one distinct contribution for financial practice, where the networks are used to construct long-short portfolios. The thesis computes a novel metric, Portfolio Reliance, which allows to see which variables the networks consider important to identify future winners and losers. A decomposition in time further uncovers that these variables change from year to year, unlike the overall, or cross-sectional, importance of the features discussed above. (The models constructed in this thesis offer comparable profitability to state-of-the-art models both in terms of mean return and Sharpe ratio, which makes them very relevant to a finance practitioner. The use of a highly liquid universe of stocks further increases the relevance of the results for trading.)   

Finally, there are three quite small contributions to the ML interpretability literature. First, the thesis shows how Integrated Gradient can be used as a global measure of feature importance, not only local. Second, it allows to better understand the sources behind superior performance of ensembles, by decomposing their interpretation into constituent models. Finally, it proposes a novel feature importance measure, that is particularly suited for interpreting stock returns predictions. 

The areas for future research are numerous. First, running all results again on simulated data would bring a greater insight into the soundness of the interpretability measures: by knowing which variables are important in the simulation, it can be inspected which interpretability metrics do a good job at uncovering them. It would also be interesting to see if different metrics are more or less suitable as the signal-to-noise ratio changes across more (less) noisy prediction tasks. Second, this thesis identifies variables with likely non-linear relationship to stock return by comparing the interpretation to linear regression. However, it could be investigated in detail what is the exact \textit{form} of the non-linearity: which interactions of variables are important and what are the exact functional forms of the relationship between the predictors and stock return found by the networks. Third, it should be investigated whether the interpretation of the networks in asset-pricing is susceptible to adversarial attacks, as is the case of much more complex networks used for image recognition \citep{ghorbani2019interpretation}. Finally, and this is likely the most thrilling area of future research, the statistical properties of feature importance measures in ML are as of yet quite unclear. More research -- e.g. in the direction of \cite{fisher2019all} -- could bring about an exciting union of statistics and interpretable machine learning.   



