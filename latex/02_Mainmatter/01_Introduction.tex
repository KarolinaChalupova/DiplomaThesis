\chapter{Introduction}
\label{chap:int}

The question of why different assets earn different returns has been occupying finance for over half a century. Yet, instead of providing a clear answer, literature proposes more and more variables that purportedly explain asset returns, accumulating \textit{hundreds} of diverse candidate answers, form exposure to underlying risks of the investment to behavioral biases of the investors themselves or imperfections in the practical functioning of the stock market. At the same time, a promising strand of literature emerges that brings machine learning (ML) methods to this financial field -- and to a great avail: machines are able to predict asset returns unprecedentedly well. However, machines are mathematically complicated and the knowledge they extract from the data thus remains hidden in their complex structure, leaving the machines unexplained and the question of what drives asset returns still unanswered. In other words, the onset of machine learning in finance brings improvements in forecasting, but does not satisfactorily account for a desire of an economist to identify and interpret the effects driving those improved results. This thesis is one of the first attempts to fill this gap. With the use of so-called machine learning interpretability methods, it uncovers which variables the machines focus on when forecasting stock returns. 

Machine learning has seen a upsurge in many fields, both academic and applied. Machines, neural networks in particular, recently achieved state-of-the-art results in image and speech recognition, object detection, game playing, language translation, often surpassing human abilities. Machines are also spreading to more ethically sensitive areas, such as car driving, credit rating, or medical diagnostics. In finance, they are applied to a wide range of tasks, including bankruptcy and bank failure prediction, bond rating, inflation forecasting and, yes, stock returns prediction. In virtually all these applications, they out-perform the more traditional approaches, such as linear regression or discriminant analysis \citep{fadlalla2001analysis}. This also attracts financial investors: in stock returns prediction, neural networks offer 2 to 4 times the profitability of the older approaches \citep{gu2020empirical, tobek2020does}.  

With this advance of artificial intelligence, a need arises to understand the machines on a deeper level, to explain their decisions -- in technical terms, to \textit{interpret} the machines. This is important for ethical, scientific and practical reasons. First, from the perspective of ethics, it is crucial to understand how a model works before using it to make decisions that affect human lives, particularly in as sensitive areas as medical diagnostics or loan eligibility. Second, from the perspective of scientific curiosity, studying how exactly artificial intelligence works can bring inspiring results. For example, interpreting neural networks used in image recognition revealed surprising similarities between the hidden layers of the networks and human retinal cortex \citep{olah2017feature}. Finally, it is necessary to interpret the machines for practical purposes: to develop, debug and improve them. For instance, by uncovering which input variables are important, a machine can be further improved by fine-tuning the vital inputs \citep{de2018advances}. To meet these needs, interpretable ML is now a quickly developing field \citep{molnar2020interpretable}. 

In finance, interpretable ML is still in its infancy. \cite{bryzgalova2019forest} arrive at a data-driven discount factor for stock prices by creatively using random forests, an interpretable ML model. \cite{li2020beyond} decompose exchange rate predictions into linear and non-linear effects and show how the variables inside the networks interact to produce exchange rate prediction. \cite{gu2020empirical, tobek2020does} use neural networks to predict stock returns, but they focus on performance and profitability rather than interpretation of the machines. They show that neural networks predict stock returns better than any other model, but the investigation into the economic forces behind these  results is very limited. This work aims to fill this void. 

This thesis is likely a first deeper dive into ML interpretability in the domain of stock returns prediction. First, this offers a data-driven perspective on the long-standing economic question of what accounts for the differences in returns between stocks. It builds on the 50 years of research in finance, by using the top variables the field has accumulated as the most robust stock returns predictors -- such as momentum, reversal, liquidity or accruals -- and uses them as inputs to neural network, forecasts the stock's return in the next month. The thesis then uses ML interpretability methods to extract insights about which of these variables influence the predicted return the most, and, importantly, in which direction. This direction is important to add economic meaning to the network: all input variables have theoretical underpinnings from prior literature, which suggest which sign (direction) each particular variable should have in order to comply with economic theory. This work is likely the first to study if machines agree or disagree with the economic mechanisms the field has proposed over the last century. Apart from this economic perspective, this thesis also offers a more applied interpretation of the networks: in particular, it uncovers which variables are important for the profitability of networks in forming long-short portfolios, that is, it shows which firm characteristics the networks rely on to identify the future winners and losers among the stocks in the market. Both these contributions are to the best of my knowledge unseen in prior literature.  

The dataset is a liquid universe of globally traded stocks, from 1990 to 2018, and offers 30 stock-level variables that are a distillation of the last 50 years of research in the field. The performance of the networks used in this thesis to predict the stock returns in the following month is at par with the state-of-the-art models, which further enhances its relevance for financial practice. The thesis studies 4 feed-forward neural networks, consisting of 1 to 4 hidden layers. This architecture is ubiquitous in many financial applications \citep{fadlalla2001analysis}, which makes the work quite relevant also to other areas than stock return prediction.    

The thesis is structured as follows: Chaper \ref{chap:lit} offers a review of existing literature, focusing first on the economic motivation of stock return predictors, then on the current state of interpretable ML in stock returns, and finally on the selection of a suitable feature importance measure. Chapter \ref{chap:met} first describes the dataset and then explains the methodology behind training, evaluating and interpreting the networks used. Chapter \ref{chap:res} gives the results, first demonstrating the predictive ability and profitability of the networks, and then diving into their interpretation. Chapter \ref{con} concludes and offers areas for further research. 
