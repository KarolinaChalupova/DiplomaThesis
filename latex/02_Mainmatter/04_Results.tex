\chapter{Results}
\label{chap:res}

\section{Performance Evaluation}
	
	\subsection{Predictive Ability}
	
		Table \ref{tab:performance} shows the predictive ability of the networks. All four nets (NN1 to NN4) are rather similar in the mean errors of their predictions. The Mean Absolute Error is around 0.075, which is a rather high portion of the variability in monthly return (mean monthly return is 0.005 with 0.051 standard deviation).  However, these relatively high prediction errors are a commonplace in stock returns prediction and are due to the low signal-to noise ratio of financial data in general. The $R^2$ shows the fraction of variability in data explained by the model, above that of the naive forecast of zero return. The values range from 0.022\% for NN4 across 0.039\% for NN3 to 0.21\% for NN1 and finally 0.25\% for NN2. These numbers are somewhat lower than in \cite{gu2020empirical}, who report $R^2$ of around 0.35\%, but discussion with the authors of \cite{tobek2020does} reveals that the $R^2$ values found in this thesis are actually commonplace in the stock returns prediction task [TODO I can't find a proper citation, nobody seems to report R2].    
		
		Figure \ref{fig:r2} shows a decomposition of $R^2$ into deciles by return: the stocks are grouped into 10 groups, corresponding to the decile of their return, and $R^2$ is calculated separately for each decile. The results show that the predictive ability of the networks is higher the higher the return: put simply, the networks are good at predicting returns of stocks that end up performing very well and bad at predicting mediocre returns. In the top 10\% of stocks (decile 100), the networks explain as much as  0.5\% to 1\% of returns variability above naive forecast of 0. The performance is similarly good in the 80th and the 90th decile, but seems to be decreasing. For the 60th, 50th, 40th decile, the performance ranges around 0, meaning that the models explain as much variance as the naive forecast of 0 around the center of the returns distribution. For 30th decile, all models actually under-perform the naive 0 forecast (negative values of $R^2$). Some models (NN1 and NN3) seem to improve in the lowest decile, (barely) climbing out of the negative $R^2$ territory. This generally indicates that the models may be better at predicting returns of winners than mediocre and poorly-performing stocks, at least in terms of the explained variance of the data.         
				
			\begin{table}
				\centering
				\input{Tables/performance.tex}
				\caption{Out-of-Sample Predictive Ability of the Networks}
				\label{tab:performance}
				\medskip
				\small
				NN1 to NN4 stand for neural networks of respective depths. All metrics are calculated out-of-sample and are defined in Section \ref{chap:model_evaluation}.  $R^2$ is in percentage points.  
			\end{table}
		
			\begin{figure}
				\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/r2.pdf}
				\caption{Decomposition of Out-of-Sample $R^2$ into Deciles}
				\label{fig:r2}
				\medskip
				\small 
				NN1 to NN4 stand for neural networks of respective depths. Out-of-sample predictions of each network are split into 10 subsets based on decile of the actual return. $R^2$ of the prediction is then calculated on the individual subsets. For example, decile 10 (100) denotes the bottom (top) 10\% of observations by return (all deciles are mutually exclusive by construction). The figure shows that all networks are able to predict high returns well (decile 80, 90 and 100), but they under-perform the naive prediction of 0 (negative $R^2$) in the low to medium return deciles (30 to 60).  
			\end{figure}
		

	\subsection{Profitability of Trading Strategy (Backtest)}
		\label{chap:backtest}
		
		\afterpage{%    % defer execution until the next page break occurs anyway
			\begin{center}
				\begin{figure}
					\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/backtest_cumreturns_models.pdf}
					\caption{Cumulative Returns on the Long-Short Portfolios}
					\label{fig:backtest_cumreturns_models}
					\medskip
					\small 
					As described in \ref{chap:met_backtest}, the portfolios are constructed by letting each network make out-of-sample prediction of returns in the next month, ordering the stocks by predicted return from highest to lowest, buying the top 10\% of stocks and short-selling the bottom 10\%. The Market portfolio is constructed by buying the entire universe of stocks available in the given month. All portfolios are equal-weighted. 
				\end{figure}
			\end{center}		
			\begin{table}
				\centering
				\input{Tables/backtest_descriptives_models.tex}
				\caption{Descriptive Statistics of Returns on Long-Short Portfolios}
				\label{tab:backtest_descriptives_models}
				\medskip
				\small 
				As described in \ref{chap:met_backtest}, the portfolios are constructed by letting each network make out-of-sample prediction of returns in the next month, ordering the stocks by predicted return from highest to lowest, buying the top 10\% of stocks and short-selling the bottom 10\%. The Market portfolio is constructed by buying the entire universe of stocks available in the given month. All portfolios are equal-weighted. 
			\end{table}
			\begin{center}	
				\begin{figure}
					\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/backtest_histogram.pdf}
					\caption{Histogram of Monthly Returns on the Long-Short Portfolio Generated by NN1}
					\label{fig:backtest_histogram}
					\medskip
					\small
					As described in \ref{chap:met_backtest}, the NN1 portfolio is constructed by letting a network make out-of-sample prediction of returns in the next month, ordering the stocks by predicted return from highest to lowest, buying the top 10\% of stocks and short-selling the bottom 10\%. The Market portfolio is constructed by buying the entire universe of stocks available in the given month. Both portfolios are equal-weighted. 
				\end{figure}
			\end{center}
			
		} % end of argument of `\afterpage` command
		
		\afterpage{%    % defer execution until the next page break occurs anyway
			\begin{center}
				\begin{figure}
					\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/backtest_cumreturns_ls.pdf}
					\caption{Cumulative Returns on Long and Short Legs of the Portfolio Generated by NN1}
					\label{fig:backtest_cumreturns_ls}
					\medskip 
					\small 
					As described in \ref{chap:met_backtest}, the NN1 portfolio is constructed by letting a network make out-of-sample prediction of returns in the next month, ordering the stocks by predicted return from highest to lowest, buying the top 10\% of stocks and short-selling the bottom 10\%. The Market portfolio is constructed by buying the entire universe of stocks available in the given month. Both portfolios are equal-weighted. 
				\end{figure}
			\end{center}		
			\begin{table}
				\centering
				\input{Tables/backtest_descriptives_ls.tex}
				\caption{Descriptive Statistics of Returns on Long-Short Portfolios Generated by NN1 in Different Capital Allocations}
				\label{tab:backtest_descriptives_ls}
				\medskip 
				\small 
				As described in \ref{chap:met_backtest}, the NN1 portfolio is constructed by letting a network make out-of-sample prediction of returns in the next month, ordering the stocks by predicted return from highest to lowest, buying the top $m$\% of stocks and short-selling the bottom $n$\%. Here, $m=n$ and takes values 0.5, 1, 5, 10 and 20, which gives respectively 8, 17, 88, 177, 177, and 355 firms in the long postiion and the same number of firms in the short position. All portfolios are equal-weighted. 
			\end{table}
			
		} % end of argument of `\afterpage` command
		
		
		Table \ref{tab:backtest_descriptives_models} Figure \ref{fig:backtest_histogram}  and describe the distribution of monthly returns earned on long-short portfolios generated by the individual networks. As described in \ref{chap:met_backtest}, the portfolios are constructed by letting a network make out-of-sample prediction of returns in the next month, ordering the stocks by predicted return from highest to lowest, buying the top 10\% of stocks and short-selling the bottom 10\%. The figures also show the market return, which serves as a benchmark. All three figures show that all the networks beat the market returns by a large margin, which gives a clear sign of their ability to identify winners and losers ex ante, resulting in a profitable long-short trading strategy. The two figures are now discussed in turn in more detail.
		
		Table \ref{tab:backtest_descriptives_models} shows the descriptive statistics of monthly returns on the long-short portfolio. While the market earns 0.5\% in an average months, the networks earn more than double: 0.9\% for the worst-performing net (NN3) and 1.4\% for the best networks (NN1 and NN4). This translates to yearly return of 12, 15, 18 and 19\% (in NN3, NN2, NN4 and NN1 respectively), which comfortably beats the 7\% earned by the market. While beating the market in average return, the networks exhibit \textit{less} return volatility at the same time: the standard deviation of the market return is 0.05, while the networks' is around 0.04. This favorable risk-return trade-off is summarized in Sharpe Ratio of around 1 (compared to market's 0.3). This gives very similar performance to the prior literature: compare the Sharpe Ratios to \cite{gu2020empirical} (in brackets): 1.16 (1.16) for NN1, 1.05 (1.15) for NN2 and  1.23 (1.35) for NN4. The only exception is NN3, which has a considerably poorer Sharpe Ratio of 0.7 (1.20). \cite{tobek2020does} report Sharpe Ratios of the networks from 0.88 to 1.58, which further confirms that the performance of the networks in this thesis is state-of-the-art. The remaining rows in Table \ref{tab:backtest_descriptives_models} summarize the skewness and kurtosis of the distributions of monthly returns: NN1 and NN3 are skewed to the right, NN3 and NN4 to the left, and the tails are generally only somewhat lighter than that of standard normal distribution for all models (kurtosis from 2.5 to 3.0). Maximum Drawdown shows the loss a trader would suffer if she started trading in 2000 and existed in the least favorable moment possible: again, the networks beat the market (-0.27 to -0.32 compared to markets' -0.58), and are close to state-of-the-art (-0.15 to -0.26 in \cite{gu2020empirical} and -0.19 to -0.38 in \cite{tobek2020does}.)   
		
		Figure \ref{fig:backtest_histogram} visual illustrate the more favorable mean, standard deviation, skewness and kurtosis of the networks' return distributions than that of market's: the mean (and also all the deciles) are more to the right, the left tail is less pronounced and the right tail is also more favorable (meaning that investors experience less terrible months and more great months than when investing to the market). The somewhat lower standard deviation is also visible in the figure. Most months see a return of 0 to 2.5\%.       
		
		Figure \ref{fig:backtest_cumreturns_models} shows the returns on the very same portfolios from a time-series perspective: it shows cumulative return earned at any time point since 2000 (beginning of the out-of-sample data). Again, market return is plotted as a benchmark. All networks consistently outperform the market (again, with a slight exception of the rather poorly performing NN3 in the early years). If trading from 2000 and exiting in 2018, an investor would pocket around 400\% of the initial investment (200\% from the poorly performing NN3 and 120\% for the market). The recessions are also visible: 2003, 2008 and 2018 saw negative returns, showing as decreasing cumulative return in the figure.      
		 
		Figure \ref{fig:backtest_cumreturns_ls} shows the decomposition of the cumulative return on the long-short portfolio\footnote{The portfolio is generated by NN1. Other architectures give similar results.} to its long and short legs. The figure also shows the market return, which serves as a benchmark. IT shows that the short-sold stocks are consistently below the market and in negative numbers, which translates to profits for the short position, while the long position is at par or better than the market. The short and long position combined result in a consistent out-performance. Also note that the short position helps to protect the portfolio in the recessions: The network was able to predict the losers if 2002 and 2008 recessions, which meant that the return of the long-short portfolio was partially insulated from the economic downturn.     
		
		
		All the backtesting performance described to this point assumes that the long (short) position is created as top (bottom) 10\%  of the stocks, as ordered by predicted return. Table \ref{tab:backtest_descriptives_ls} describes how the results change if the focus of the positions would be more or less narrow (resulting in lower or higher number of firms in both positions: 8, 17, 88, 177, 177, and 355 from right to left).\footnote{The portfolio is generated by NN1. Other architectures give similar results.} The results reviewed until now are in the column 10--10 (meaning 10\% of the market is bought and 10\% short-sold). The column 20--20 represents the situation if the positions would be distributed into more firms (broader portfolios) and the columns 5--5 1--1 and 0.5--0.5 represent progressively narrower positions (less firms bought and sold). As is expected, the mean returns are higher for the narrower portfolios (as much as 44\% yearly for 1--1), but this comes at the cost of higher standard deviation (more volatile returns) and worse Maximum Drawdown. It appears that the optimum risk-return trade-off is with 5--5, which gives the highest Sharpe Ratio of 1.2. While all variants still beat the market in mean return, only 10--10 and 20--20 are less volatile than the market, which is because there are too little firms in the smaller portfolios  to ensure a decrease in volatility (e.g., 17 firms in the long and 17 in the short position for the 1--1).

	
\section{Feature Importance}
	\label{chap:global_feature_importance}
	
	Figures \ref{fig:ig_ensemble} and \ref{fig:pr_ensemble} shows the values of feature importance for all features, as measured respectively by Global Integrated Gradients and Portfolio Reliance. The results for the two measures are now discussed in turn, and then compared to each other in \ref{fig:ig_pr_comparison}.
	
	Figure \ref{fig:ig_ensemble} shows importance of all features, for all models as measured by Integrated Gradients. As discussed in Chapters \ref{chap:lit} and \ref{chap:met}, Integrated Gradients show how a marginal change in input values (features) influences output of the model (the predicted return). Intuitively, if the output changes a lot with a small change in given feature, the feature is considered important for the prediction. The figure shows that all neural networks agree on the importance to a large extent. In both panels, the order of features as they appear is the same and is determined by the mean value across all the 5 models (4 networks and 1 linear regression), so that the most (least) important features appear on top (bottom) of the figure. Panel (\subref{fig:ig_blues}) shows the values of the Global Integrated Gradient. 
	
	The size of the measure has a ready interpretation: it shows how on average the predicted return changes due to the marginal change in input. A useful property of the Integrated Gradient is that the sum across features equals the predicted return. For example, 52-Week-High, the most important feature, on average changes the predicted return by around 0.3 percentage points. This is quite high, as the mean  \footnote{A limitation of the measure is that the direction of the change is not visible (the average is taken from absolute values as discussed in \ref{chap:met}.}   
	  
	
	\begin{figure}	
		\centering		
		\begin{subfigure}[t]{\textwidth}
			\includegraphics[width=\textwidth]{Figures/ig_blues.pdf}
			\caption{Values of Global Integrated Gradient}
			\label{fig:ig_blues}
		\end{subfigure}
		
		\begin{subfigure}[t]{\textwidth}
			\includegraphics[width=\textwidth]{Figures/ig_order.pdf}
			\caption{Order of Features by Importance}
			\label{fig:ig_order}
		\end{subfigure}
		\caption{Feature Importance Measured with Global Integrated Gradient}
		\medskip
		\small
		Column LR shows linear regression and columns NN1 to NN4 the neural networks of respective depths. Panel (\subref{fig:ig_blues}) shows values of the Global Integrated Gradient for all features, for all models.  Panel (\subref{fig:ig_order}) shows ordering of features by their importance, with bright colors corresponding to high order. Label 1 --- bright yellow (30 --- black) corresponds to most (least) important feature in given model, as measured by highest (lowest) Global Integrated Gradient. 
		\label{fig:ig_ensemble}
	\end{figure}
	
	
	\begin{figure}	
		\centering		
		\begin{subfigure}[t]{\textwidth}
			\includegraphics[width=\textwidth]{Figures/pr_blues.pdf}
			\caption{Values of Portfolio Reliance}
			\label{fig:pr_blues}
		\end{subfigure}
		
		\begin{subfigure}[t]{\textwidth}
			\centering
			\includegraphics[width=\textwidth]{Figures/pr_order.pdf}
			\caption{Order of Features by Importance}
			\label{fig:pr_order}
		\end{subfigure}
		\caption{Feature Importance Measured with Portfolio Reliance.}
		\medskip
		\small
		Column LR shows linear regression and columns NN1 to NN4 the neural networks of respective depths. Panel (\subref{fig:pr_blues}) shows values of the Portfolio Reliance for all features, for all models.  Panel (\subref{fig:pr_order}) shows ordering of features by their importance, with bright colors corresponding to high order. Label 1 --- bright yellow (30 --- black) corresponds to most (least) important feature in given model, as measured by highest (lowest) Portfolio Reliance. 
		\label{fig:pr_ensemble}
	\end{figure}

	\begin{figure}
		\centering
		\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/ig_pr_comparison.pdf}
		\caption{Comparison of Feature Importance Measured with Integrated Gradients and with Portfolio Reliance}
		\label{fig:ig_pr_comparison}
		\medskip
		\small 
		Column LR shows linear regression and columns NN1 to NN4 the neural networks of respective depths. The values of Integrated Gradients are the same as in Panel (\subref{fig:ig_blues}) of Figure \ref{fig:ig_ensemble}, the values of Portfolio Reliance are the same as in Panel (\subref{fig:pr_blues}) of Figure \ref{fig:pr_ensemble}. The only difference is that all columns are divided by their maximum value so as to unify the scale across the different measures. Values 1 correspond to the most important feature in given column, values of 0 to the least important feature. 
	\end{figure}


\subsection{Feature Importance In Time}

	This section decomposes the main results (presented in Section \ref{chap:global_feature_importance}) into different time periods. Recall from Section \ref{chap:train_regularize_tune} that any model (e.g., NN1) is completely re-trained every year, as new data arrives and the training set thus expands (and validation and testing set rolls forward accordingly). The main results are average across all these independent instances of models in time. This section offers a decomposition. Technically, there are two reasons why feature importance may change in time. Either, the true data-generating process changed (i.e., the relationship between returns and firm characteristics changed), or the model converged to a different solution. TODO the simulations could help distinguish the two, go back to this. 

	\begin{figure}	
		\centering		
		\begin{subfigure}[t]{\textwidth}
			\includegraphics[width=\textwidth]{Figures/ig_relative.pdf}
			\caption{Main Result}
			\label{fig:ig_time_main}
		\end{subfigure}
		
		\begin{subfigure}[t]{\textwidth}
			\centering
			\includegraphics[width=\textwidth]{Figures/ig_time_relative.pdf}
			\caption{Time Decomposition}
			\label{fig:ig_time_relative}
		\end{subfigure}
		\caption{Feature Importance in Time as Measured by Global Integrated Gradients}
		\label{fig:ig_time}
		\medskip
		\small
		Column LR shows linear regression and columns NN1 to NN4 the neural networks of respective depths. Both panels show relative values of Global Integrated Gradient: all values are divided by the value for the most important feature, so as to unify the scale across the different models and time periods. 
	\end{figure}

	\begin{figure}	
		\centering		
		\begin{subfigure}[t]{\textwidth}
			\includegraphics[width=\textwidth]{Figures/pr_relative.pdf}
			\caption{Main Result}
			\label{fig:pr_time_main}
		\end{subfigure}
		
		\begin{subfigure}[t]{\textwidth}
			\centering
			\includegraphics[width=\textwidth]{Figures/pr_time_relative.pdf}
			\caption{Time Decomposition}
			\label{fig:pr_time_relative}
		\end{subfigure}
		\caption{Feature Importance in Time as Measured by Portfolio Reliance}
		\label{fig:pr_time}
		\medskip
		\small
		Column LR shows linear regression and columns NN1 to NN4 the neural networks of respective depths. Both panels show relative values of Portfolio Reliance: all values are divided by the value for the most important feature, so as to unify the scale across the different models and time periods. 
	\end{figure}

\subsection{Feature Importance Across Random Seeds}

	This section decomposes the main results into the component models (random seeds) of the ensemble models presented in Section \ref{chap:global_feature_importance}. Recall from Section \ref{chap:ensembling} that all the models presented in this thesis are ensemble models: each model is an average of 10 different versions of itself, where the versions (called random seed) differ in the initialization of values of the model's weights at the beginning of the training. (Reasons and methodological details of this procedure, called \textit{ensembling}, are discussed in Section \ref{chap:ensembling}.) Decomposing the results into random seeds is useful for several reasons. From one perspective, it can be perceived as a robustness check to see whether small changes in parameter values at the beginning of the optimization influence the results. From the other perspective, it offers a peek into why ensemble models work well: small errors made by the component models average out and the truth arises in the between. As \cite{fisher2019all} write in the title of their paper: "All models are wrong, but many are useful". From this perspectives, if different random seeds find different relationships in the data, the ensemble model can combine this knowledge, which offers it an edge over any of its sub-components. 
	
	Figures \ref{fig:ig_seeds} and \ref{fig:pr_seeds} show how the results of the ensemble models are decomposed into the random seeds. 

	\begin{figure}	
		\centering		
		\begin{subfigure}[t]{\textwidth}
			\includegraphics[width=\textwidth]{Figures/ig_relative_without_lr.pdf}
			\caption{Main Result}
			\label{fig:ig_seeds_main}
		\end{subfigure}
		
		\begin{subfigure}[t]{\textwidth}
			\centering
			\includegraphics[width=\textwidth]{Figures/ig_seeds_relative.pdf}
			\caption{Decomposition into Random Seeds}
			\label{fig:ig_seeds_relative}
		\end{subfigure}
		\caption{Feature Importance across Random Seeds as Measured by Integrated Gradients}
		\label{fig:ig_seeds}
		\medskip
		\small
		Column LR shows linear regression and columns NN1 to NN4 the neural networks of respective depths. Both panels show relative values of Global Integrated Gradient: all values are divided by the value for the most important feature, so as to unify the scale across the different models. 
	\end{figure}
	
	\begin{figure}	
		\centering		
		\begin{subfigure}[t]{\textwidth}
			\includegraphics[width=\textwidth]{Figures/pr_relative_without_lr.pdf}
			\caption{Main Result}
			\label{fig:pr_seeds_main}
		\end{subfigure}
		
		\begin{subfigure}[t]{\textwidth}
			\centering
			\includegraphics[width=\textwidth]{Figures/pr_seeds_relative.pdf}
			\caption{Decomposition Into Random Seeds}
			\label{fig:pr_seeds_relative}
		\end{subfigure}
		\caption{Feature Importance across Random Seeds as Measured by Portfolio Reliance}
		\label{fig:pr_seeds}
		\medskip
		\small
		Column LR shows linear regression and columns NN1 to NN4 the neural networks of respective depths. Both panels show relative values of Portfolio Reliance: all values are divided by the value for the most important feature, so as to unify the scale across the different models.
	\end{figure}
	
	