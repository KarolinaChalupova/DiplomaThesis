\chapter{Results}
\label{chap:res}

\section{Performance Evaluation}

	\subsection{Predictive Accuracy}
	
		Table \ref{tab:performance} shows the predictive accuracy of the ensemble models. 		
			\begin{table}
				\centering
				\input{Tables/performance.tex}
				\caption{Predictive Accuracy of the Models}
				\label{tab:performance}
			\end{table}

	\subsection{Backtest Using Long-Short Portfolios}
		\label{chap:backtest}
	
		Figure \ref{fig:backtest_cumreturns_models} and Table \ref{tab:backtest_descriptives_models} shows the returns on long-short portfolios generated on the out-of-sample predictions of the models.
		
		
		\afterpage{%    % defer execution until the next page break occurs anyway 		
			\begin{center}
				\begin{figure}
					\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/backtest_cumreturns_models.pdf}
					\caption{Cumulative Returns on Long-Short Portfolios (10-10 Allocation)}
					\label{fig:backtest_cumreturns_models}
				\end{figure}
			\end{center}
			\begin{table}
				\resizebox{\textwidth}{!}{\input{Tables/backtest_descriptives_models.tex}}
				\caption{Descriptive Statistics of Returns on Long-Short Portfolios (10-10 Allocation)}
				\label{tab:backtest_descriptives_models}
			\end{table}
		} % end of argument of `\afterpage` command
	
		Figure \ref{fig:backtest_cumreturns_ls} shows the decompositions of the cumulative return on the long-short portfolio\footnote{The portfolio is generated by NN1. Other architectures give similar results.} to its long and short lags. The figure also shows the return on the entire universe of stocks, which serves as a benchmark.  
			
		Table \ref{tab:backtest_descriptives_ls} and Figure \ref{fig:backtest_histogram} describe the distribution of the monthly returns earned on the long-short portfolio.\footnote{The portfolio is generated by NN1. Other architectures give similar results.} The table 
		
		
		\afterpage{%    % defer execution until the next page break occurs anyway
			\begin{center}
				\begin{figure}
					\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/backtest_cumreturns_ls.pdf}
					\caption{Cumulative Returns on Long and Short Legs of the Long-Short Portfolio (NN1, 10-10 Allocation)}
					\label{fig:backtest_cumreturns_ls}
				\end{figure}
			\end{center}		
			\begin{center}	
				\begin{figure}
						\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/backtest_histogram.pdf}
						\caption{Histogram of Monthly Returns on the Long-Short Portfolio (NN1, 10-10 Allocation)}
						\label{fig:backtest_histogram}
				\end{figure}
			\end{center}
			\begin{table}
				\centering
				\input{Tables/backtest_descriptives_ls.tex}
				\caption{Descriptive Statistics of Returns Earned on Long-Short Portfolios in Different Long-Short Allocations (NN1)}
				\label{tab:backtest_descriptives_ls}
			\end{table}
			
		} % end of argument of `\afterpage` command
		
	\section{Global Feature Importance}
	
		\subsection{Integrated Gradients}
	
		Figure \ref{fig:ig_ensemble} shows global feature importance in all models, as measured by Global Integrated Gradient. 
		
		\begin{figure}	
			\centering		
			\begin{subfigure}[t]{\textwidth}
				\includegraphics[width=\textwidth]{Figures/ig_blues.pdf}
				\caption{Values of Global Integrated Gradient}
				\label{fig:ig_blues}
			\end{subfigure}
		
			\begin{subfigure}[t]{\textwidth}
				\centering
				\includegraphics[width=\textwidth]{Figures/ig_relative.pdf}
				\caption{Relative Values of Global Integrated Gradient}
				\label{fig:ig_relative}
			\end{subfigure}
			\caption{Global Feature Importance Measured with Integrated Gradients.}
			\medskip
			\small
			Column LR shows linear regression and columns NN1 to NN5 the neural networks of respective depths. Panel (\subref{fig:ig_blues}) shows values of the Global Integrated Gradient for all features, for all models.  Panel (\subref{fig:ig_relative} shows relative importance of the features: values of Global Integrated Gradient are divided by the value for the most important feature, so as to unify the scale across the different models.
			\label{fig:ig_ensemble}
		\end{figure}
		
		
		Figure \ref{ig:order} shows the ordering of features by importance, as measured by the Global Integrated Gradient. For each model, the 30 features are ordered from most to least important (highest to lowest values of Global Integrated Gradient), and assigned values from 1 to 30 (1 for most important). Plotting these orders gives a heatmap of global feature importance for all models, which shows which features are relatively more important than others.\footnote{Integrated Gradients are particularly suited for forming such heatmap, as they distinguish well even between unimportant features, unlike Model Reliance (Figure \ref{fig:mr_ensemble}) and Portfolio Reliance (Figure\ref{fig:pr_ensemble}.}
		
		\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{Figures/ig_order.pdf}
			\caption{Order of Features by Importance}
			\label{fig:ig_order}
			\medskip
			\small
			Column LR shows linear regression and columns NN1 to NN5 the neural networks of respective depths.
			For each model, the figure shows the ordering of features by their global importance, as defined by Global Integrated Gradient. The colors show the relative importance of the features, as ordered by the size of Global Integrated Gradient of the feature. Label 1 --- bright yellow (30 --- black) corresponds to most (least) important feature in given model, as measured by highest (lowest) Global Integrated Gradient.
		\end{figure}
		
		\subsection{Model Reliance}
	
			Figure \ref{fig:mr_ensemble} shows global feature importance in all models, as measured by Model Reliance.
			
			\begin{figure}	
				\centering		
				\begin{subfigure}[t]{\textwidth}
					\includegraphics[width=\textwidth]{Figures/mr_blues.pdf}
					\caption{Values of Model Reliance}
					\label{fig:mr_blues}
				\end{subfigure}
				
				\begin{subfigure}[t]{\textwidth}
					\centering
					\includegraphics[width=\textwidth]{Figures/mr_relative.pdf}
					\caption{Relative Values of Model Reliance}
					\label{fig:mr_relative}
				\end{subfigure}
				\caption{Global Feature Importance Measured with Model Reliance.}
				\medskip
				\small
				Column LR shows linear regression and columns NN1 to NN5 the neural networks of respective depths. Panel (\subref{fig:mr_blues}) shows values of Model Reliance for all features, for all models.  Panel (\subref{fig:mr_relative} shows relative importance of the features: values of Global Integrated Gradient are divided by the value for the most important feature, so as to unify the scale across the different models.
				\label{fig:mr_ensemble}
			\end{figure}
	
	\subsection{Portfolio Reliance}
	
		Figure \ref{fig:pr_ensemble} shows global feature importance in all models, as measured by Portfolio Reliance.
		
		\begin{figure}	
			\centering		
			\begin{subfigure}[t]{\textwidth}
				\includegraphics[width=\textwidth]{Figures/pr_blues.pdf}
				\caption{Values of Portfolio Reliance}
				\label{fig:pr_blues}
			\end{subfigure}
			
			\begin{subfigure}[t]{\textwidth}
				\centering
				\includegraphics[width=\textwidth]{Figures/pr_relative.pdf}
				\caption{Relative Values of Portfolio Reliance}
				\label{fig:pr_relative}
			\end{subfigure}
			\caption{Global Feature Importance Measured with Portfolio Reliance.}
			\medskip
			\small
			Column LR shows linear regression and columns NN1 to NN5 the neural networks of respective depths. Panel (\subref{fig:pr_blues}) shows values of Model Reliance for all features, for all models.  Panel (\subref{fig:pr_relative} shows relative importance of the features: values of Global Integrated Gradient are divided by the value for the most important feature, so as to unify the scale across the different models.
			\label{fig:pr_ensemble}
		\end{figure}
		
		
	\section{Robustness Checks for Global Feature Importance}
		\label{chap:robustness}
		
		Two kinds of robustness checks of global feature importance are performed: robustness in time and robustness across random seeds. First, robustness in time shows how importance of individual features changes if the same model is trained (and tested) on different time windows of data. A change in importance of a feature in time can happen for two reasons: either, the model converges to a different solution, or the true data-generating relationship changed in time. The latter effect is expected to be less pronounced \cite{gu2020empirical}, which allows to focus on model convergence. As all instances of a single model in time are trained independently on each other, robustness in time is crucial: if the results are stable in time, it means that 5 independent instances of the model, trained on 5 different time windows, agree on them.  
		
		Technically, the robustness in time is checked as follows: Recall from Section \ref{chap:train_regularize_tune} that the train-validation-test split is performed 5 separate times. The first time the models are trained on data from 1990 to 2001 (12 years), validated on data from 2002 to 2013 (12 years) and tested on data from 2014. The remaining 4 train-validation-test splits are performed in the same manner, except each time the training window expands by 1 year, and the validation and test splits roll forward appropriately (i.e., also by 1 year). This means, that the train-validation-test splits can be summarized as 12-12-1, 13-12-1, 14-12-1, 15-12-1 and 16-12-1, where the number represents amount of years in the corresponding dataset. Also recall from Section \ref{chap:train_regularize_tune} that the models are completely re-trained for each such split. This means there are 5 instances of the same model in time (independently trained). The results in the previous section come from the 16-12-1 split (tested on 2018). This section shows how the model changes for the remaining splits.  
		
		Second, robustness across random seeds [TODO expand]
		
		Figures \ref{fig:ig_robustness}, \ref{fig:mr_robustness}, \ref{fig:pr_robustness} show how global feature importance, as measured, respectively, by Integrated Gradients, Model Reliance and Portfolio Reliance, changes with time and with random seed. For each model, the measure is calculated on 5 different time periods (2014 to 2018, both inclusive).
		
		
		\begin{figure}	
			\centering		
			\begin{subfigure}[t]{\textwidth}
				\includegraphics[width=\textwidth]{Figures/ig_time_relative.pdf}
				\caption{Robustness across Time}
				\label{fig:ig_time_relative}
			\end{subfigure}
			
			\begin{subfigure}[t]{\textwidth}
				\centering
				\includegraphics[width=\textwidth]{Figures/ig_seeds_relative.pdf}
				\caption{Robustness across Random Seeds}
				\label{fig:ig_seeds_relative}
			\end{subfigure}
			\caption{Robustness of Global Feature Importance Measured with Integrated Gradients.}
			\medskip
			\small
			Column LR shows linear regression and columns NN1 to NN5 the neural networks of respective depths. In Panel (\subref{fig:ig_time_relative}), for each model, Global Integrated Gradient is calculated on 5 different time periods (2014 to 2018, both inclusive). At the beginning of each year, the model is re-estimated and the measure is then calculated on the following 1-year window of out-of-sample data. In Panel (\subref{fig:ig_seeds_relative}), each ensemble model is decomposed into its 9 random seeds (for details on ensembling using random seeds, see Subsection \ref{chap:ensembling}). For each seed model, Global Integrated Gradient is calculated separately on out-of-sample data in 2018. For both panels, the plotted values are \textit{relative} to unify the scale across the different models (column-wise division by maximum value is performed).
			\label{fig:ig_robustness}
		\end{figure}
	
	\begin{figure}	
		\centering		
		\begin{subfigure}[t]{\textwidth}
			\includegraphics[width=\textwidth]{Figures/mr_time_relative.pdf}
			\caption{Robustness across Time}
			\label{fig:mr_time_relative}
		\end{subfigure}
		
		\begin{subfigure}[t]{\textwidth}
			\centering
			\includegraphics[width=\textwidth]{Figures/mr_seeds_relative.pdf}
			\caption{Robustness across Random Seeds}
			\label{fig:mr_seeds_relative}
		\end{subfigure}
		\caption{Robustness of Global Feature Importance Measured with Model Reliance.}
		\medskip
		\small
		Column LR shows linear regression and columns NN1 to NN5 the neural networks of respective depths. In Panel (\subref{fig:mr_time_relative}), for each model, Model Reliance is calculated on 5 different time periods (2014 to 2018, both inclusive). At the beginning of each year, the model is re-estimated and the measure is then calculated on the following 1-year window of out-of-sample data. In Panel (\subref{fig:mr_seeds_relative}), each ensemble model is decomposed into its 9 random seeds (for details on ensembling using random seeds, see Subsection \ref{chap:ensembling}). For each seed model,  Model Reliance is calculated separately on out-of-sample data in 2018. For both panels, the plotted values are \textit{relative} to unify the scale across the different models (column-wise division by maximum value is performed).
		\label{fig:mr_robustness}
	\end{figure}
		
	\begin{figure}	
		\centering		
		\begin{subfigure}[t]{\textwidth}
			\includegraphics[width=\textwidth]{Figures/pr_time_relative.pdf}
			\caption{Robustness across Time}
			\label{fig:pr_time_relative}
		\end{subfigure}
		
		Column LR shows linear regression and columns NN1 to NN5 the neural networks of respective depths. In Panel (\subref{fig:pr_time_relative}), for each model, Portfolio Reliance is calculated on 5 different time periods (2014 to 2018, both inclusive). At the beginning of each year, the model is re-estimated and the measure is then calculated on the following 1-year window of out-of-sample data. For both panels, the plotted values are \textit{relative} to unify the scale across the different models (column-wise division by maximum value is performed).
		\label{fig:pr_robustness}
	\end{figure}
	
		
	\section{Simulation Results}
	
		All the above results are here computed again, on simulated data. This has the vital advantage that the true relationship between predictors and stock returns is known (controlled wholly by the researcher), which implies that it is possible to better establish the \textit{source} of the patterns the models learn from the data. Specifically, only the first three simulated features (C1, C2 and C3) have a relationship with stock returns, the other are by construction independent of them. The only difference between the simulation and the reality is in the \textit{dataset}\footnote{The precise way to arrive to simulated data is described in \ref{chap:simulations}}. Otherwise, the models are trained and measures calculated in the exact same manner as the actual ones. 
		
		Figures \ref{fig:sim_ig_ensemble_relative}, \ref{fig:sim_mr_ensemble_relative} and \ref{fig:sim_pr_ensemble_relative} show global feature importance as measured, respectively, by Integrated Gradients, Model Reliance and Portfolio Reliance. All three simulations give a rather surprising result: all models seem to give importance to different features simply by chance and have no relationship to the true data-generating process (plotted in column \textit{True}) an all figures. 
			
		Does this mean that the models are useless in predicting the stock returns? Certainly not: we have seen that the ability of the models to predict returns profitably is great (Section \ref{chap:backtest}). Does this mean that the selected interpretability measures are inadequately chosen? Again, this cannot be the case: the robustness results show that the measures describe the learned patterns consistently, irrespective of time and random initialization (Section \ref{chap:robustness}). Indeed, as we have seen, the actual results are consistent, no matter the model architecture, time window, random initialization or measure (Integrated Gradients and Portfolio Reliance agree well). So what is the root cause of the difference between the simulated and actual results? If the issue cannot be in the models or the measures, it must be in the data. The only material difference between the simulated and the real data is in variances of the features: while the real predictors have markedly different variances, as seen in Figures \ref{fig:histograms} and \ref{fig:standard_deviation}, the simulated features are drawn from the same distribution and thus have very similar sample variances. Indeed, in the real data, we can see that the important features have high variances, while on the simulated data, where the variances are the same, the importance is close to arbitrary.
		
		This brings us to a crucial finding: important features are important because they have high variance. On the first sight, the finding is trivial: features with higher variance offer a higher opportunity to learn (similarly in linear regression, significance of a predictor is the lower the lower the variance of the predictor). However, there is much more than this. Importantly, it means that the interpretability measures tell more about the models than they do about the true relationship between predictors and returns: they \textit{do} say which variables the \textit{models} focus on to make a prediction (Integrated Gradient) or to yield profitable trading strategies (Portfolio Reliance). It appears they do that by relying on information from high-variance features. However, this does not imply that the low-variance features do not have a \textit{true} relationship to returns. In other words, the true relationship seems to be masked by the strength of high-variance features (see how the true relationship is not uncovered for the simulated results). Thus, the results tell more about the models than about the true relationship. To put it differently, using neural networks to model stock returns and interpreting them has all the sense from the perspective of a quantitative analyst 
		
		
		
		\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{Figures/sim_ig_ensemble_relative.pdf}
			\caption{Global Feature Importance on Simulated Data, as Measured with Integrated Gradients}
			\label{fig:sim_ig_ensemble_relative}
			\medskip
			\small
			Column LR shows linear regression and columns NN1 to NN5 the neural networks of respective depths. Column True shows the true simulated relationship. For each model, the figure shows the values of Global Integrated Gradient for each feature (relative values are given: all values are divided by the value for the most important feature, so as to unify the scale across the different models.)
		\end{figure}
		
		\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{Figures/sim_mr_ensemble_relative.pdf}
			\caption{Global Feature Importance on Simulated Data, as Measured with Model Reliance}
			\label{fig:sim_mr_ensemble_relative}
			\medskip
			\small
			Column LR shows linear regression and columns NN1 to NN5 the neural networks of respective depths. Column True shows the true simulated relationship. For each model, the figure shows the values of Model Reliance for each feature (relative values are given: all values are divided by the value for the most important feature, so as to unify the scale across the different models.)
		\end{figure}
	
		\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{Figures/sim_pr_ensemble_relative.pdf}
			\caption{Global Feature Importance on Simulated Data, as Measured with Portfolio Reliance}
			\label{fig:sim_pr_ensemble_relative}
			\medskip
			\small
			Column LR shows linear regression and columns NN1 to NN5 the neural networks of respective depths. Column True shows the true simulated relationship. For each model, the figure shows the values of Portfolio Reliance for each feature (relative values are given: all values are divided by the value for the most important feature, so as to unify the scale across the different models.)
		\end{figure}
	